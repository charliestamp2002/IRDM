{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee0ba5ce",
   "metadata": {},
   "source": [
    "# Information Retreival: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6c2241",
   "metadata": {},
   "source": [
    "# L1\n",
    "\n",
    "## What is information Retrieval?\n",
    "\n",
    "“Information retrieval is a field concerned with\n",
    "the structure, analysis, organization, storage,\n",
    "and retrieval of information.\"\n",
    "\n",
    "*Gerald Salton*\n",
    "\n",
    "## ***What is structure in the context of IR?***\n",
    "\n",
    "+ In the context of Information Retrieval (IR), “structure” refers to the way information or data is organized to facilitate efficient retrieval and relevance ranking. It plays a critical role in enabling IR systems to understand, index, and search through vast amounts of data.\n",
    "\n",
    "### Document Structure: \n",
    "\n",
    "+ Refers to the way individual documents are organized. For example, books may have chapters, sections, and paragraphs, while web pages may have HTML tags indicating headings, links, or metadata.\n",
    "\n",
    "### Collection Structure: \n",
    "\n",
    "+ A collection structure refers to the organization, composition, and properties of the dataset or document corpus being searched. The collection structure plays a critical role in determining how the retrieval system processes, indexes, and retrieves relevant information.\n",
    "\n",
    "## Information Retrieval: Organisation\n",
    "\n",
    "***Examples may include:***\n",
    "\n",
    "1) Cataloguing/tagging\n",
    "\n",
    "2) Topic\n",
    "\n",
    "3) Location\n",
    "\n",
    "4) Likes\n",
    "\n",
    "## Information Retrieval: Storage\n",
    "\n",
    "***Examples may include:***\n",
    "\n",
    "1) Inverted index\n",
    "\n",
    "2) Distributed storage\n",
    "\n",
    "3) Peer-to-peer\n",
    "\n",
    "4) Privacy, security, censorship\n",
    "\n",
    "## Important concepts in retrieval are *effectiveness*, *precision*, and *recall*\n",
    "\n",
    "### *Effectiveness*: \n",
    "\n",
    "Effectiveness refers to the overall quality or success of a retrieval system in meeting the user’s information needs. It measures how well the system retrieves relevant documents and avoids irrelevant ones. How do you determine the overall quality or success of a retrieval system? Via **Evaluation Metrics**.\n",
    "\n",
    "### *Precision*:\n",
    "\n",
    "Precision = (Number of retrieved docs that are relevant) / (total number of retrieved docs)\n",
    "\n",
    "\n",
    "\n",
    "### *Recall*:\n",
    "\n",
    "Recall = (Number of retrieved docs that are relevant) / (Total number of relevant docs) \n",
    "\n",
    "### Important Terms: *Crawling, Indexing, Boolean Search, and Ranking*\n",
    "\n",
    "\n",
    "**Crawling** is the process of automatically discovering, fetching, and storing web pages or documents for indexing. It is often the first step in building an IR system.\n",
    "\n",
    "**Indexing** involves organizing and storing data retrieved during crawling in a structured format that enables fast and efficient search and retrieval.\n",
    "\n",
    "**Boolean search** uses logical operators to formulate complex queries for retrieving documents that satisfy specific conditions.\n",
    "\n",
    "Operators can include: **AND**, **NOT**, **OR**...\n",
    "\n",
    "**Ranking** refers to the process of ordering retrieved documents by relevance to the user’s query.\n",
    "\n",
    "***How do these fit together in IR?***\n",
    "\n",
    "\n",
    "\t1.\tCrawling: Collects raw data (documents or web pages) for the system.\n",
    "\t2.\tIndexing: Organizes the data into a searchable structure.\n",
    "\t3.\tBoolean Search: Enables users to craft specific queries for retrieval.\n",
    "\t4.\tRanking: Prioritizes the retrieved results to show the most relevant ones first.\n",
    "    \n",
    "## Representation of Documents: *Bag of Words*\n",
    "\n",
    "The *Bag of Words* (BoW) representation is a simple and widely used method in Information Retrieval (IR) and Natural Language Processing (NLP) for representing text data. It treats a document as a “bag” of its words, disregarding grammar, word order, and syntax but preserving the frequency of each word.\n",
    "\n",
    "## Example of Bag of Words Representation\n",
    "\n",
    "## Corpus:\n",
    "1. \"The cat sat on the mat.\"\n",
    "2. \"The dog barked at the cat.\"\n",
    "\n",
    "## Vocabulary:\n",
    "Unique words: `[\"The\", \"cat\", \"sat\", \"on\", \"mat\", \"dog\", \"barked\", \"at\"]`\n",
    "\n",
    "## Bag of Words Representation:\n",
    "\n",
    "| **Word**  | **Doc 1 (Counts)** | **Doc 2 (Counts)** |\n",
    "|-----------|--------------------|--------------------|\n",
    "| The       | 2                  | 2                  |\n",
    "| cat       | 1                  | 1                  |\n",
    "| sat       | 1                  | 0                  |\n",
    "| on        | 1                  | 0                  |\n",
    "| mat       | 1                  | 0                  |\n",
    "| dog       | 0                  | 1                  |\n",
    "| barked    | 0                  | 1                  |\n",
    "| at        | 0                  | 1                  |\n",
    "\n",
    "## Vector Representation:\n",
    "- **Doc 1**: `[2, 1, 1, 1, 1, 0, 0, 0]`\n",
    "- **Doc 2**: `[2, 1, 0, 0, 0, 1, 1, 1]`\n",
    "\n",
    "\n",
    "# What is Relevance? \n",
    "\n",
    "**Relevance** is the “correspondence” between information needs (queries) and information items (documents, webpages, images etc). But, the exact meaning of relevance depends\n",
    "on applications:\n",
    "\n",
    "Is it useful?\n",
    "Is it topical? \n",
    "= interesting\n",
    "= ?\n",
    "\n",
    "***Predicting relevance is the central goal of IR***\n",
    "\n",
    "## What is a Retrieval Model? \n",
    "\n",
    "A **retrieval model** in the context of Information Retrieval (IR) refers to a formal framework or set of algorithms used to determine the relevance of documents in response to a user’s query. It defines how documents and queries are represented, how their relevance is computed, and how search results are ranked.\n",
    "\n",
    "### Key components of a Retrieval Model: \n",
    "\n",
    "1.\t**Document Representation**: How documents are represented in the system (e.g., as bags of words, vectors, embeddings, etc.).\n",
    "2.\t**Query Representation**: How user queries are modeled, which might be similar to or different from document representations.\n",
    "3.\t**Relevance Estimation**: The method for computing the degree of relevance between a query and a document.\n",
    "4.\t**Ranking Mechanism**: Determines the order in which documents are presented to the user, typically based on their relevance scores.\n",
    "\n",
    "**Categories of Retrieval Models include**: *Exact-Matching Models*, *Vector Space Models*, *Probabilistic Models*, *Neural Retrieval Models*, *Hybrid Models*.\n",
    "\n",
    "## How do we define Relevance?\n",
    "\n",
    "Defining **relevance** in Information Retrieval (IR) involves assessing how well a document satisfies a user’s information need. Several factors contribute to this definition, as relevance is not just about matching query terms. Below are detailed explanations of topicality, novelty, freshness, and authority, key factors in determining relevance:\n",
    "\n",
    "**Topicality**: Refers to how closely a document’s content matches the topic or subject of the user’s query.\n",
    "\n",
    "**Novelty**: Refers to the degree to which a document provides new or unique information that has not already been encountered by the user.\n",
    "\n",
    "**Freshness**: Refers to how recent or up-to-date the information in a document is.\n",
    "\n",
    "**Authority**: Refers to the credibility, trustworthiness, or expertise of the source or author of a document.\n",
    "\n",
    "*To summarise*: \n",
    "\n",
    "+\t**Topicality** ensures that the document matches the query.\n",
    "+\t**Novelty** avoids redundancy and adds unique value.\n",
    "+\t**Freshness** provides time-sensitive information.\n",
    "+\t**Authority** ensures the information is credible and trustworthy.\n",
    "\n",
    "*An example*: \n",
    "\n",
    "For a query like “climate change impacts in 2024”:\n",
    "\n",
    "+ Topicality ensures documents discuss climate change.\n",
    "+ Novelty ensures unique insights are retrieved.\n",
    "+ Freshness prioritizes recent 2024 studies or reports.\n",
    "+ Authority ensures reliable sources, like government reports or peer-reviewed papers, rank higher.\n",
    "\n",
    "# Exact Matching in Information Retrieval\n",
    "\n",
    "## Core Concepts of Exact Matching\n",
    "\n",
    "1. **Keyword Matching**:\n",
    "   - Documents are retrieved if they contain the exact keywords specified in the query.\n",
    "   - **Example**:\n",
    "     - Query: \"machine learning\"\n",
    "     - Retrieved document: Contains the exact phrase \"machine learning.\"\n",
    "\n",
    "2. **Boolean Matching**:\n",
    "   - Uses logical operators like AND, OR, and NOT to define conditions for document retrieval.\n",
    "   - **Examples**:\n",
    "     - Query: `\"machine AND learning\"` retrieves documents that contain both \"machine\" and \"learning.\"\n",
    "     - Query: `\"machine OR learning\"` retrieves documents that contain either \"machine\" or \"learning\" or both.\n",
    "\n",
    "3. **Phrase Matching**:\n",
    "   - Retrieves documents containing a specific sequence of words.\n",
    "   - **Example**:\n",
    "     - Query: `\"deep neural networks\"`\n",
    "     - Only documents with the exact phrase \"deep neural networks\" are retrieved.\n",
    "\n",
    "4. **Wildcard Matching**:\n",
    "   - Allows partial term matching using special symbols (e.g., `*` or `?`).\n",
    "   - **Example**:\n",
    "     - Query: `\"comput*\"` matches \"computer,\" \"computing,\" \"computation,\" etc.\n",
    "\n",
    "5. **Field-Specific Matching**:\n",
    "   - Matches terms in specific fields of a document, such as the title, abstract, or author name.\n",
    "   - **Example**:\n",
    "     - Query: `title:\"machine learning\"`\n",
    "\n",
    "---\n",
    "\n",
    "## Strengths of Exact Matching\n",
    "\n",
    "1. **Simplicity**:\n",
    "   - Straightforward to implement and understand.\n",
    "   - Does not require complex algorithms or semantic analysis.\n",
    "\n",
    "2. **Precision**:\n",
    "   - High precision when queries are well-constructed, as only documents explicitly matching the query terms are retrieved.\n",
    "\n",
    "3. **Deterministic**:\n",
    "   - Provides consistent results since matching is based on strict rules.\n",
    "\n",
    "4. **Speed**:\n",
    "   - Efficient for small-scale collections with well-defined indexing.\n",
    "\n",
    "---\n",
    "\n",
    "## Limitations of Exact Matching\n",
    "\n",
    "1. **Vocabulary Mismatch**:\n",
    "   - Cannot handle cases where users use synonyms or related terms.\n",
    "   - **Example**:\n",
    "     - Query: \"car\" will not retrieve documents containing only \"automobile.\"\n",
    "\n",
    "2. **Lack of Flexibility**:\n",
    "   - Overly rigid; slight variations in wording or phrasing can lead to missed documents.\n",
    "   - **Example**:\n",
    "     - Query: `\"AI research\"` may miss documents using \"artificial intelligence research.\"\n",
    "\n",
    "3. **Context Ignorance**:\n",
    "   - Ignores the semantic relationships and meaning of words.\n",
    "   - **Example**:\n",
    "     - Query: \"bass\" could retrieve documents about fish or music, with no understanding of context.\n",
    "\n",
    "4. **No Ranking or Relevance Scoring**:\n",
    "   - Documents are not ranked based on their relevance but are often returned in arbitrary or predefined order (e.g., by document ID or timestamp).\n",
    "\n",
    "5. **Challenges with Long Queries**:\n",
    "   - Long or complex queries may yield no results if all terms are required to match.\n",
    "   \n",
    " \n",
    "# Ranked Retrieval: \n",
    "   \n",
    "**Ranked retrieval** is a method used in Information Retrieval to organize the results of a search query by their relevance to the user’s query. Unlike exact matching, which often provides unranked or unordered results, ranked retrieval ensures that the most relevant documents appear at the top of the result list.\n",
    "\n",
    "# Vector Space Model (VSM)\n",
    "\n",
    "The **Vector Space Model (VSM)** is a mathematical and conceptual framework used in **Information Retrieval (IR)** to represent and compare documents and queries as vectors in a multi-dimensional space. It allows retrieval and ranking of documents based on their relevance to a query.\n",
    "\n",
    "---\n",
    "\n",
    "## Core Concepts of the Vector Space Model\n",
    "\n",
    "1. **Vector Representation**:\n",
    "   - Each document and query is represented as a vector in a space where each dimension corresponds to a unique term (word) in the vocabulary.\n",
    "   - The value in each dimension is a weight, often based on the term's importance in the document or query (e.g., TF-IDF).\n",
    "\n",
    "2. **Similarity Measurement**:\n",
    "   - The relevance of a document to a query is determined by the similarity between their respective vectors.\n",
    "   - Similarity is commonly measured using **cosine similarity**, which computes the angle between the two vectors.\n",
    "\n",
    "3. **Partial Matching**:\n",
    "   - VSM allows for partial matching, meaning documents can still be considered relevant even if they don't contain all query terms.\n",
    "\n",
    "---\n",
    "\n",
    "## Mathematics of the Vector Space Model\n",
    "\n",
    "### **Document Representation**:\n",
    "Given a vocabulary of \\( n \\) terms, each document $ D_i $ is represented as a vector:\n",
    "$$\n",
    "\\vec{D_i} = [w_{i1}, w_{i2}, ..., w_{in}]\n",
    "$$\n",
    "where $ w_{ij} $ is the weight of term $ j $ in document $ i $.\n",
    "\n",
    "### **Query Representation**:\n",
    "Similarly, the query $ Q $ is represented as:\n",
    "$$\n",
    "\\vec{Q} = [q_1, q_2, ..., q_n]\n",
    "$$\n",
    "where $ q_j $ is the weight of term $ j $ in the query.\n",
    "\n",
    "---\n",
    "\n",
    "### **Term Weighting**:\n",
    "Common weighting schemes include:\n",
    "\n",
    "- **Term Frequency (TF)**: Counts the occurrences of a term in a document.\n",
    "$$\n",
    "\\text{TF}(t, D) = \\frac{\\text{Frequency of term } t \\text{ in } D}{\\text{Total terms in } D}\n",
    "$$\n",
    "\n",
    "- **Inverse Document Frequency (IDF)**: Measures how unique a term is across the corpus.\n",
    "$$ \n",
    "\\text{IDF}(t) = \\log\\frac{\\text{Total number of documents}}{\\text{Number of documents containing } t}\n",
    "$$\n",
    "\n",
    "- **TF-IDF**: Combines TF and IDF to prioritize terms that are frequent in a document but rare in the corpus.\n",
    "$$\n",
    "\\text{TF-IDF}(t, D) = \\text{TF}(t, D) \\times \\text{IDF}(t)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Cosine Similarity**:\n",
    "Cosine similarity measures the cosine of the angle between the document vector \\( \\vec{D_i} \\) and query vector \\( \\vec{Q} \\):\n",
    "$$\n",
    "\\text{Cosine Similarity} = \\cos(\\theta) = \\frac{\\vec{D_i} \\cdot \\vec{Q}}{\\|\\vec{D_i}\\| \\|\\vec{Q}\\|}\n",
    "$$\n",
    "where:\n",
    "- $ \\vec{D_i} \\cdot \\vec{Q} $: Dot product of the document and query vectors.\n",
    "- $ \\|\\vec{D_i}\\| $: Magnitude of the document vector.\n",
    "- $ \\|\\vec{Q}\\| $: Magnitude of the query vector.\n",
    "\n",
    "---\n",
    "\n",
    "## Steps in Using the Vector Space Model\n",
    "\n",
    "1. **Preprocessing**:\n",
    "   - Tokenize text into terms.\n",
    "   - Remove stopwords and apply stemming or lemmatization.\n",
    "\n",
    "2. **Vectorization**:\n",
    "   - Construct term-document and term-query matrices.\n",
    "   - Calculate term weights (e.g., using TF-IDF).\n",
    "\n",
    "3. **Similarity Computation**:\n",
    "   - Compute cosine similarity between the query vector and each document vector.\n",
    "\n",
    "4. **Ranking**:\n",
    "   - Rank documents in descending order of their similarity scores.\n",
    "\n",
    "---\n",
    "\n",
    "## Example\n",
    "\n",
    "### **Corpus**:\n",
    "1. **Doc 1**: \"The cat sat on the mat.\"\n",
    "2. **Doc 2**: \"The dog barked at the cat.\"\n",
    "\n",
    "### **Vocabulary**:\n",
    "- Terms: `[\"the\", \"cat\", \"sat\", \"on\", \"mat\", \"dog\", \"barked\", \"at\"]`\n",
    "\n",
    "### **Term Frequencies (TF)**:\n",
    "\n",
    "| **Term**  | **Doc 1 (TF)** | **Doc 2 (TF)** |\n",
    "|-----------|----------------|----------------|\n",
    "| the       | 0.33           | 0.33           |\n",
    "| cat       | 0.17           | 0.17           |\n",
    "| sat       | 0.17           | 0.00           |\n",
    "| on        | 0.17           | 0.00           |\n",
    "| mat       | 0.17           | 0.00           |\n",
    "| dog       | 0.00           | 0.17           |\n",
    "| barked    | 0.00           | 0.17           |\n",
    "| at        | 0.00           | 0.17           |\n",
    "\n",
    "---\n",
    "\n",
    "## Advantages of the Vector Space Model\n",
    "\n",
    "1. **Flexible Matching**:\n",
    "   - Supports partial matches, increasing recall.\n",
    "\n",
    "2. **Quantifiable Similarity**:\n",
    "   - Provides a numeric relevance score for ranking.\n",
    "\n",
    "3. **Conceptually Simple**:\n",
    "   - Intuitive and easy to implement.\n",
    "   \n",
    "# Zipf's Law for Term Frequency\n",
    "\n",
    "**Zipf's Law** is a principle that describes the statistical distribution of word frequencies in natural language text. It states that the frequency of a word is **inversely proportional to its rank** in the frequency distribution. In simpler terms, the most frequent word appears roughly twice as often as the second most frequent word, three times as often as the third most frequent word, and so on.\n",
    "\n",
    "---\n",
    "\n",
    "## Formal Definition\n",
    "\n",
    "If:\n",
    "- \\( f \\): frequency of a word,\n",
    "- \\( r \\): rank of the word in the frequency distribution (1 for the most frequent word, 2 for the second most frequent, etc.),\n",
    "- \\( C \\): a constant of proportionality,\n",
    "\n",
    "then Zipf's Law is expressed as:\n",
    "$$\n",
    "f(r) \\propto \\frac{1}{r}\n",
    "$$\n",
    "or equivalently:\n",
    "$$\n",
    "f(r) = \\frac{C}{r}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Characteristics of Zipf's Law\n",
    "\n",
    "1. **Power-Law Distribution**:\n",
    "   - Zipf's Law is a specific case of a power-law distribution. It implies that a few words are extremely frequent, while most words are rare.\n",
    "   - The relationship can be plotted on a log-log scale, where the rank and frequency form approximately a straight line.\n",
    "\n",
    "2. **High-Frequency Words**:\n",
    "   - A small number of words (e.g., \"the,\" \"and,\" \"is\") make up a large proportion of the text.\n",
    "\n",
    "3. **Long Tail**:\n",
    "   - Most words in a corpus occur very infrequently (e.g., rare or domain-specific terms).\n",
    "\n",
    "---\n",
    "\n",
    "## Example\n",
    "\n",
    "Consider the following text:\n",
    "\n",
    "- **Text**: \"the cat sat on the mat and the dog barked\"\n",
    "- **Word Frequencies**:\n",
    "  - \"the\" (4 occurrences)\n",
    "  - \"cat\" (1 occurrence)\n",
    "  - \"sat\" (1 occurrence)\n",
    "  - \"on\" (1 occurrence)\n",
    "  - \"mat\" (1 occurrence)\n",
    "  - \"and\" (1 occurrence)\n",
    "  - \"dog\" (1 occurrence)\n",
    "  - \"barked\" (1 occurrence)\n",
    "\n",
    "### Rank-Frequency Table:\n",
    "\n",
    "| **Rank (r)** | **Word** | **Frequency (f)** | $$ f \\propto \\frac{1}{r} $$ |\n",
    "|--------------|----------|-------------------|----------------------------|\n",
    "| 1            | the      | 4                 | $$ 4 = \\frac{C}{1} $$      |\n",
    "| 2            | cat      | 1                 | $$ 1 = \\frac{C}{2} $$      |\n",
    "| 3            | sat      | 1                 | $$ 1 = \\frac{C}{3} $$      |\n",
    "| 4            | on       | 1                 | $$ 1 = \\frac{C}{4} $$      |\n",
    "| ...          | ...      | ...               | ...                        |\n",
    "\n",
    "Here, the most frequent word \"the\" has 4 occurrences, which is roughly twice as frequent as the second most frequent word, and so on.\n",
    "\n",
    "---\n",
    "\n",
    "## Applications of Zipf's Law in Information Retrieval\n",
    "\n",
    "1. **Stopword Removal**:\n",
    "   - Words like \"the,\" \"and,\" and \"is\" (highly frequent) often provide little information and can be removed to reduce noise.\n",
    "\n",
    "2. **Index Optimization**:\n",
    "   - Understanding term frequencies can guide the construction of efficient inverted indexes by prioritizing common terms.\n",
    "\n",
    "3. **TF-IDF Weighting**:\n",
    "   - Zipf's Law underpins the **Inverse Document Frequency (IDF)** component of TF-IDF, emphasizing terms that are rare across documents.\n",
    "\n",
    "4. **Vocabulary Analysis**:\n",
    "   - Helps understand language characteristics and guides the selection of features for NLP tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## Limitations of Zipf's Law\n",
    "\n",
    "1. **Empirical Approximation**:\n",
    "   - While Zipf's Law holds well for natural language, it is not an exact law and may deviate for specific corpora or domains.\n",
    "\n",
    "2. **Assumes Independence**:\n",
    "   - Ignores relationships between words, such as phrases or semantic dependencies.\n",
    "\n",
    "3. **Long Tail Handling**:\n",
    "   - Rare terms in the \"long tail\" may require different processing strategies, such as smoothing or data augmentation.\n",
    "   \n",
    "# Inverse Document Frequency (IDF)\n",
    "\n",
    "**Inverse Document Frequency (IDF)** is a numerical statistic used in **Information Retrieval (IR)** to measure the importance or rarity of a term across a collection of documents. It is a core component of the widely-used **TF-IDF** weighting scheme, which combines **Term Frequency (TF)** with IDF to rank terms based on their relevance.\n",
    "\n",
    "---\n",
    "\n",
    "## Core Concept\n",
    "\n",
    "- Terms that appear frequently across many documents (e.g., \"the,\" \"and\") are less informative because they are common.\n",
    "- Terms that appear in fewer documents (e.g., \"quantum mechanics,\" \"photosynthesis\") are more informative and significant in identifying the content of a document.\n",
    "- IDF gives higher weights to less common terms and lower weights to more common terms.\n",
    "\n",
    "---\n",
    "\n",
    "## Formula\n",
    "\n",
    "The IDF of a term $t$ is calculated as:\n",
    "$$\n",
    "\\text{IDF}(t) = \\log\\frac{N}{1 + \\text{DF}(t)}\n",
    "$$\n",
    "where:\n",
    "- $N$: Total number of documents in the collection.\n",
    "- $\\text{DF}(t)$: Number of documents containing the term $t$ (document frequency).\n",
    "- $1 + \\text{DF}(t)$: Adding 1 to avoid division by zero for terms that do not appear in any document.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Points\n",
    "\n",
    "1. **High IDF**:\n",
    "   - Terms that appear in very few documents have a high IDF, indicating their importance or uniqueness.\n",
    "   - Example: If \"quantum mechanics\" appears in 2 out of 1,000 documents, its IDF is high.\n",
    "\n",
    "2. **Low IDF**:\n",
    "   - Common terms (like \"the,\" \"and,\" \"is\") have a low IDF, reflecting their ubiquity and lack of discriminative power.\n",
    "   - Example: If \"the\" appears in all 1,000 documents, its IDF is close to zero.\n",
    "\n",
    "3. **Logarithmic Scaling**:\n",
    "   - The logarithm dampens the effect of very high document frequencies to avoid over-penalizing common terms.\n",
    "\n",
    "---\n",
    "\n",
    "## Example Calculation\n",
    "\n",
    "### Given:\n",
    "- Total documents ($N$): 10,000\n",
    "- Document Frequency of \"machine\" ($\\text{DF}(\\text{machine})$): 1,000\n",
    "- Document Frequency of \"learning\" ($\\text{DF}(\\text{learning})$): 50\n",
    "\n",
    "### IDF Values:\n",
    "\n",
    "1. For \"machine\":\n",
    "   $$\n",
    "   \\text{IDF}(\\text{machine}) = \\log\\frac{10,000}{1 + 1,000} = \\log\\frac{10,000}{1,001} \\approx 1\n",
    "   $$\n",
    "\n",
    "2. For \"learning\":\n",
    "   $$\n",
    "   \\text{IDF}(\\text{learning}) = \\log\\frac{10,000}{1 + 50} = \\log\\frac{10,000}{51} \\approx 2.3\n",
    "   $$\n",
    "\n",
    "\"Learning\" has a higher IDF than \"machine,\" indicating that it is a rarer and more informative term.\n",
    "\n",
    "---\n",
    "\n",
    "## Why Use IDF?\n",
    "\n",
    "1. **Improves Relevance**:\n",
    "   - Helps prioritize terms that are more likely to differentiate between documents.\n",
    "\n",
    "2. **Reduces Noise**:\n",
    "   - Common terms (stopwords) have little impact on ranking since their IDF is low.\n",
    "\n",
    "3. **Balances Term Frequency**:\n",
    "   - Works with **TF (Term Frequency)** to ensure that rare terms with high TF in a document are emphasized.\n",
    "\n",
    "---\n",
    "\n",
    "## Applications of IDF\n",
    "\n",
    "1. **TF-IDF Weighting**:\n",
    "   - Used to rank documents in search engines based on relevance.\n",
    "   - $$\n",
    "   \\text{TF-IDF}(t, D) = \\text{TF}(t, D) \\times \\text{IDF}(t)\n",
    "   $$\n",
    "\n",
    "2. **Text Classification**:\n",
    "   - Features with high TF-IDF values are often used as input for machine learning models.\n",
    "\n",
    "3. **Topic Modeling**:\n",
    "   - Helps identify key terms that define a topic in a corpus.\n",
    "\n",
    "4. **Summarization**:\n",
    "   - Identifies critical terms in documents for summarization tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## Limitations of IDF\n",
    "\n",
    "1. **Static Nature**:\n",
    "   - Traditional IDF is calculated based on a fixed corpus, which can be outdated for dynamic or growing datasets.\n",
    "\n",
    "2. **Fails with Short Documents**:\n",
    "   - In small or sparse datasets, IDF may not provide meaningful weights.\n",
    "\n",
    "3. **Ignores Semantics**:\n",
    "   - IDF focuses on term frequency and does not consider the meaning or context of terms.\n",
    "\n",
    "4. **Over-Penalization of Common Terms**:\n",
    "   - Very high document frequencies may result in negligible weights for moderately useful terms.\n",
    "   \n",
    "# Cosine Similarity\n",
    "\n",
    "**Cosine Similarity** is a measure used in **Information Retrieval (IR)** and **Natural Language Processing (NLP)** to determine the similarity between two vectors. It calculates the cosine of the angle between two non-zero vectors in a multi-dimensional space, where smaller angles indicate higher similarity.\n",
    "\n",
    "---\n",
    "\n",
    "## Definition\n",
    "\n",
    "Cosine Similarity measures the orientation (not the magnitude) of two vectors, making it particularly useful when comparing documents where the magnitude of the vectors (e.g., word frequency counts) is less important than their direction.\n",
    "\n",
    "### Formula\n",
    "For two vectors $ \\vec{A} $ and $ \\vec{B} $, cosine similarity is defined as:\n",
    "$$\n",
    "\\text{Cosine Similarity} = \\cos(\\theta) = \\frac{\\vec{A} \\cdot \\vec{B}}{\\|\\vec{A}\\| \\|\\vec{B}\\|}\n",
    "$$\n",
    "Where:\n",
    "- $ \\vec{A} \\cdot \\vec{B} $: Dot product of vectors $ \\vec{A} $ and $ \\vec{B} $.\n",
    "- $ \\|\\vec{A}\\| $: Magnitude (or norm) of vector $ \\vec{A} $, calculated as:\n",
    "  $$\n",
    "  \\|\\vec{A}\\| = \\sqrt{\\sum_{i=1}^n A_i^2}\n",
    "  $$\n",
    "- $ \\|\\vec{B}\\| $: Magnitude of vector $ \\vec{B} $.\n",
    "\n",
    "---\n",
    "\n",
    "## Steps to Compute Cosine Similarity\n",
    "\n",
    "1. **Vector Representation**:\n",
    "   - Represent the documents or queries as vectors. Each dimension corresponds to a term in the vocabulary, and the vector values are typically weights like term frequency (TF) or TF-IDF.\n",
    "\n",
    "2. **Compute the Dot Product**:\n",
    "   - Multiply corresponding components of the vectors and sum the results:\n",
    "     $$\n",
    "     \\vec{A} \\cdot \\vec{B} = \\sum_{i=1}^n A_i \\cdot B_i\n",
    "     $$\n",
    "\n",
    "3. **Compute the Magnitudes**:\n",
    "   - Calculate the Euclidean norm of each vector:\n",
    "     $$\n",
    "     \\|\\vec{A}\\| = \\sqrt{\\sum_{i=1}^n A_i^2}, \\quad \\|\\vec{B}\\| = \\sqrt{\\sum_{i=1}^n B_i^2}\n",
    "     $$\n",
    "\n",
    "4. **Calculate Cosine Similarity**:\n",
    "   - Divide the dot product by the product of the magnitudes.\n",
    "\n",
    "---\n",
    "\n",
    "## Example\n",
    "\n",
    "### Vectors:\n",
    "Let:\n",
    "- $ \\vec{A} = [1, 2, 3] $\n",
    "- $ \\vec{B} = [4, 5, 6] $\n",
    "\n",
    "### Step 1: Compute the Dot Product:\n",
    "$$\n",
    "\\vec{A} \\cdot \\vec{B} = (1 \\cdot 4) + (2 \\cdot 5) + (3 \\cdot 6) = 4 + 10 + 18 = 32\n",
    "$$\n",
    "\n",
    "### Step 2: Compute the Magnitudes:\n",
    "- $ \\|\\vec{A}\\| $:\n",
    "  $$\n",
    "  \\|\\vec{A}\\| = \\sqrt{1^2 + 2^2 + 3^2} = \\sqrt{1 + 4 + 9} = \\sqrt{14}\n",
    "  $$\n",
    "- $ \\|\\vec{B}\\| $:\n",
    "  $$\n",
    "  \\|\\vec{B}\\| = \\sqrt{4^2 + 5^2 + 6^2} = \\sqrt{16 + 25 + 36} = \\sqrt{77}\n",
    "  $$\n",
    "\n",
    "### Step 3: Compute Cosine Similarity:\n",
    "$$\n",
    "\\text{Cosine Similarity} = \\frac{\\vec{A} \\cdot \\vec{B}}{\\|\\vec{A}\\| \\|\\vec{B}\\|} = \\frac{32}{\\sqrt{14} \\cdot \\sqrt{77}} \\approx \\frac{32}{32.86} \\approx 0.974\n",
    "$$\n",
    "\n",
    "The cosine similarity is approximately 0.974, indicating that the vectors are highly similar.\n",
    "\n",
    "---\n",
    "\n",
    "## Properties of Cosine Similarity\n",
    "\n",
    "1. **Range**:\n",
    "   - The result lies between -1 and 1:\n",
    "     - **1**: Vectors are identical in direction.\n",
    "     - **0**: Vectors are orthogonal (completely dissimilar).\n",
    "     - **-1**: Vectors point in opposite directions (rare in practice for text data).\n",
    "\n",
    "2. **Magnitude-Independent**:\n",
    "   - Cosine similarity focuses on direction, not magnitude, making it robust for comparing documents of different lengths.\n",
    "\n",
    "3. **Sparsity**:\n",
    "   - Works well with sparse vectors, as is common in text data.\n",
    "\n",
    "---\n",
    "\n",
    "## Applications\n",
    "\n",
    "1. **Document Similarity**:\n",
    "   - Measure the similarity between documents for tasks like plagiarism detection or duplicate detection.\n",
    "\n",
    "2. **Search Ranking**:\n",
    "   - Rank documents based on their cosine similarity to a query.\n",
    "\n",
    "3. **Clustering**:\n",
    "   - Group similar documents in clustering algorithms like k-means.\n",
    "\n",
    "4. **Recommendation Systems**:\n",
    "   - Compare user profiles or item attributes to recommend similar items.\n",
    "\n",
    "---\n",
    "\n",
    "## Advantages\n",
    "\n",
    "1. **Robust to Length Differences**:\n",
    "   - Accounts for differences in document or query lengths, focusing only on term distribution.\n",
    "\n",
    "2. **Efficient to Compute**:\n",
    "   - Especially efficient with sparse data and optimized libraries.\n",
    "\n",
    "3. **Widely Used**:\n",
    "   - A standard similarity measure in IR, NLP, and machine learning.\n",
    "\n",
    "---\n",
    "\n",
    "## Limitations\n",
    "\n",
    "1. **Ignores Term Relationships**:\n",
    "   - Considers only the terms explicitly in the vector, ignoring synonyms or semantic relationships.\n",
    "\n",
    "2. **Sensitive to Vector Sparsity**:\n",
    "   - Works best when vectors are well-populated; sparse vectors may yield less meaningful results.\n",
    "\n",
    "3. **Binary or Weighted Values**:\n",
    "   - Performance depends on how terms are weighted (e.g., binary, TF, or TF-IDF).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c758671",
   "metadata": {},
   "source": [
    "# L2: Prof. Lampos part (First Hour) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f4e857",
   "metadata": {},
   "source": [
    "## Text Processing: \n",
    "\n",
    "A document unit (book, book chapter, article, paragraph, sentence, tweet, search query, fixed window of terms...) will go through a number of steps: \n",
    "\n",
    "+ Parsing \n",
    "+ Tokenisation\n",
    "+ Normalisation\n",
    "+ Stop Word Removal\n",
    "+ Lemmatisation\n",
    "+ Stemming\n",
    "\n",
    "It must be noted that *Stop Word Removal*, *Lemmatisation*, and *Stemming* are optional and this order is not necessarily rigid. For example, tokenisation may come before parsing. \n",
    "\n",
    "### Parsing: \n",
    "\n",
    "If the file is not raw text, e.g. JSON, HTML. \n",
    "\n",
    "### Tokenisation: \n",
    "\n",
    "The task of chopping up a document unit into pieces, called tokens\n",
    "\n",
    "    Sentence: “They won’t let you fly, but they might let you sing.”\n",
    "    Token:  [They] [won’t] [let] [you] [fly] [,] [but] [they] [might] [let] [you] [sing] [.]\n",
    "    \n",
    "**Tokens** need to be turned to **terms**, i.e. processed tokens that will be\n",
    "maintained in our vocabulary index.\n",
    "\n",
    "### Normalisation:\n",
    "\n",
    "The process of canonicalising tokens so that during indexing matches\n",
    "occur despite of superficial diﬀerences in the character sequences.\n",
    "\n",
    "### Stop Word Removal: \n",
    "\n",
    "Extremely common (very frequent) words that do not add to the meaningof a document unit, but exact defini,on depends on the set of decisions we make (linked to the target task) in order to identify stop words.\n",
    "\n",
    "*Examples*: “the”, “an”, “to”, “so”, “then”.\n",
    "\n",
    "*Benefits*: reduces number of features / dimensionality and helps derive\n",
    "models that can generalise beYer, saves storage / memory space.\n",
    "\n",
    "*Issues*: might remove some meaning from the text\n",
    "e.g. “flights to London” — if we remove “to” as a stop word, then we don’t\n",
    "know whether this text snippet is about flights “to” or “from” London.\n",
    "\n",
    "‣ Could be determined using a predefined list and/or automa,cally, e.g.\n",
    "the most frequent terms in very large corpus\n",
    "\n",
    "‣ Bag-of-words models (each term is considered in isola,on) could benefit\n",
    "from stop word removal, but modern language models (e.g. BERT or GPT\n",
    "variants) might not as stop words can add to the seman,c interpreta,on\n",
    "of text\n",
    "\n",
    "‣ Should we remove stop words? Depends on the method used and the\n",
    "target task. Most of the ,mes the downstream task accuracy can be\n",
    "measured, and we can actually see whether removing stop words helps\n",
    "or not and how much.\n",
    "\n",
    "### Lemmatisation: \n",
    "\n",
    "Returns the base (*dictionary*) form of a word, which is known as the lemma.\n",
    "    \n",
    "    + “organises” or “organising” to “organise”\n",
    "    + “cars” to “car”\n",
    "    + “saw” to “see” (if “saw” is a verb)\n",
    "   \n",
    "+  Does things “properly”, i.e. requires a complete vocabulary and morphological analysis (needs to know what part of speech is the target word for example), aiming to remove inflectional endings only.\n",
    "\n",
    "### Stemming: \n",
    "\n",
    "Crude heuristic process that uses a stemmer (stemming algorithm) in an\n",
    "attempt to reduce inflected (or derived) words / tokens to their word\n",
    "stem (root form) — the stem, i.e. the output of a stemmer is very often not a vocabulary word. \n",
    "\n",
    "    + “cars” to “car”\n",
    "    + “organises” or “organising” to “organis”\n",
    "    + “story” or “stories” to “stori”\n",
    "    \n",
    "# Zipfian Distribution\n",
    "\n",
    "The **Zipfian Distribution** models the frequency of a word or term $f(r)$ in a dataset, based on its rank $r$ when terms are ordered by frequency. It is given by:\n",
    "\n",
    "$$\n",
    "f(r) = \\frac{C}{r^s}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $f(r)$: Frequency of the term at rank $r$.\n",
    "- $C$: Normalization constant, ensuring that the probabilities sum to 1 across all ranks.\n",
    "- $r$: Rank of the term (1 for the most frequent term, 2 for the second most frequent, etc.).\n",
    "- $s$: Exponent that determines the skew of the distribution (commonly $s \\approx 1$ for natural language text).\n",
    "\n",
    "---\n",
    "\n",
    "## Normalization Constant ($C$)\n",
    "\n",
    "To ensure that the frequencies or probabilities sum to 1 across all ranks $r = 1, 2, ..., N$, the normalization constant $C$ is computed as:\n",
    "$$\n",
    "C = \\frac{1}{\\sum_{r=1}^N \\frac{1}{r^s}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $N$: Total number of unique terms in the dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Points\n",
    "\n",
    "1. **Zipf's Law as a Special Case**:\n",
    "   - When $s = 1$, the distribution simplifies to Zipf's Law:\n",
    "     $$\n",
    "     f(r) = \\frac{C}{r}\n",
    "     $$\n",
    "\n",
    "2. **Interpretation**:\n",
    "   - The term at rank $r = 1$ (the most frequent term) has the highest frequency.\n",
    "   - As $r$ increases, $f(r)$ decreases exponentially, meaning terms of higher rank appear less frequently.\n",
    "\n",
    "3. **Applications**:\n",
    "   - Models word frequency in natural language.\n",
    "   - Describes power-law distributions in various domains (e.g., city populations, web page visits).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670248e0",
   "metadata": {},
   "source": [
    "# L2: Probabilistic Retrieval Models \n",
    "\n",
    "## Language Models: \n",
    "\n",
    "• A language model is a probability distribution over strings of text.\n",
    "\n",
    "• How likely is a given string in a given language?\n",
    "\n",
    "• Consider probabilities for the following strings:\n",
    "\n",
    "    • p1 = P(“language model”)\n",
    "    • p2 = P(“probability distribution”)\n",
    "    • p3 = P(“a language model is a probability distribution”)\n",
    "    \n",
    "• Every possible string has some probability.\n",
    "• Depends on the language we are modelling\n",
    "\n",
    "### What is a Unigram Language Model?\n",
    "\n",
    "A **unigram language model** assumes that each word in a document (or query) is generated independently of every other word. In this model, the probability of a sequence of words is the product of the probabilities of the individual words:\n",
    "\n",
    "$$\n",
    "P(w_1, w_2, \\dots, w_n) = P(w_1) \\cdot P(w_2) \\cdot \\dots \\cdot P(w_n)\n",
    "$$\n",
    "\n",
    "Where $P(w_i)$ is the probability of the word $w_i$ in the model. This assumption of independence simplifies calculations and is what makes it a \"unigram\" model.\n",
    "\n",
    "---\n",
    "\n",
    "### How is it used in Information Retrieval?\n",
    "\n",
    "In **Information Retrieval (IR)**, unigram language models are often employed to calculate the probability of a query $Q = \\{q_1, q_2, \\dots, q_m\\}$ given a document $D = \\{d_1, d_2, \\dots, d_n\\}$. The core idea is to rank documents based on how likely they are to generate the given query.\n",
    "\n",
    "This likelihood is calculated using the **language model of the document**. Each document is treated as a \"bag of words,\" and the probability of the query is computed as:\n",
    "\n",
    "$$\n",
    "P(Q|D) = \\prod_{i=1}^{m} P(q_i|D)\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- $P(q_i|D)$: The probability of the query word $q_i$ occurring in document $D$.\n",
    "\n",
    "### Bigram Language Model\n",
    "\n",
    "A **bigram language model** is an extension of the unigram model that takes into account word-to-word dependencies. Instead of assuming that each word is independent of the others, it models the conditional probability of a word based on the previous word in the sequence. This provides a simplistic way to include some context into the language model.\n",
    "\n",
    "---\n",
    "\n",
    "### Definition\n",
    "\n",
    "In a bigram language model, the probability of a sequence of words $w_1, w_2, \\dots, w_n$ is calculated as:\n",
    "\n",
    "$$\n",
    "P(w_1, w_2, \\dots, w_n) = P(w_1) \\cdot P(w_2|w_1) \\cdot P(w_3|w_2) \\cdot \\dots \\cdot P(w_n|w_{n-1})\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- $P(w_1)$ is the probability of the first word.\n",
    "- $P(w_i|w_{i-1})$ is the conditional probability of word $w_i$ given the previous word $w_{i-1}$.\n",
    "\n",
    "---\n",
    "\n",
    "### How is it Used in Information Retrieval?\n",
    "\n",
    "In **Information Retrieval (IR)**, a bigram language model can be applied to better capture dependencies between words in a query and a document. This is particularly useful for queries that involve phrases, where the order of words matters.\n",
    "\n",
    "For a query $Q = \\{q_1, q_2, \\dots, q_m\\}$ and a document $D = \\{d_1, d_2, \\dots, d_n\\}$, the likelihood of the query under the document's language model is calculated as:\n",
    "\n",
    "$$\n",
    "P(Q|D) = P(q_1|D) \\cdot P(q_2|q_1, D) \\cdot P(q_3|q_2, D) \\cdot \\dots \\cdot P(q_m|q_{m-1}, D)\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- $P(q_i|q_{i-1}, D)$ is the conditional probability of $q_i$ given $q_{i-1}$ under the document language model.\n",
    "- This helps account for word pair (bigram) probabilities in the document and query.\n",
    "\n",
    "---\n",
    "\n",
    "### Three main approaches Language MOdels (LM) in IR: \n",
    "\n",
    "\n",
    "• **Query likelihood model**: P(Q|M_D)\n",
    "\n",
    "• What is the probability to generate the given query, given a document language model?\n",
    "\n",
    "• **Document likelihood model**: P(D|M_Q)\n",
    "\n",
    "• What is the probability to generate the given document, given a query language model?\n",
    "\n",
    "• **Divergence model**: D(P(M_D) || P(M_Q))\n",
    "\n",
    "• How ”close” are 2 statistical models?\n",
    "\n",
    "\n",
    "### 1. Query Likelihood Model (QLM)\n",
    "\n",
    "The **Query Likelihood Model** is a probabilistic framework used in **language modeling for information retrieval**. In this approach, the goal is to rank documents based on the probability that a document’s language model generates the given query.\n",
    "\n",
    "#### Core Idea\n",
    "Rank documents based on $P(Q|D)$, the probability of the query $Q$ given a document $D$.\n",
    "\n",
    "#### Mathematical Representation\n",
    "Using the **unigram model** assumption (independence between query terms), $P(Q|D)$ is computed as:\n",
    "\n",
    "$$\n",
    "P(Q|D) = \\prod_{i=1}^{m} P(q_i|D)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $Q = \\{q_1, q_2, \\dots, q_m\\}$ is the query with $m$ terms.\n",
    "- $P(q_i|D)$ is the probability of query term $q_i$ under the document’s language model.\n",
    "\n",
    "#### Estimating $P(q_i|D)$\n",
    "This is often done using **Maximum Likelihood Estimation (MLE)** with smoothing to handle zero probabilities:\n",
    "- **Unsmoothed MLE**:\n",
    "  $$\n",
    "  P(q_i|D) = \\frac{\\text{Count}(q_i, D)}{|D|}\n",
    "  $$\n",
    "  Where:\n",
    "  - $\\text{Count}(q_i, D)$: Number of times $q_i$ occurs in $D$.\n",
    "  - $|D|$: Total number of terms in $D$.\n",
    "\n",
    "- **Smoothing**: Common techniques like **Jelinek-Mercer** or **Dirichlet smoothing** are used to account for unseen terms.\n",
    "\n",
    "#### Key Applications\n",
    "- Ranking documents based on their likelihood of generating the query.\n",
    "- Simple and effective for many IR tasks.\n",
    "\n",
    "#### Limitations\n",
    "- Assumes independence between query terms.\n",
    "- Relies on accurate estimation of $P(q_i|D)$, which can be challenging for short documents or rare terms.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Document Likelihood Model (DLM)\n",
    "\n",
    "The **Document Likelihood Model** flips the roles of the query and document compared to the Query Likelihood Model. Instead of ranking documents based on the likelihood of generating a query, it ranks queries based on their likelihood of generating a given document.\n",
    "\n",
    "#### Core Idea\n",
    "Rank queries $Q$ based on $P(D|Q)$, the probability of the document $D$ given the query $Q$.\n",
    "\n",
    "#### Mathematical Representation\n",
    "Using **Bayes’ Theorem**:\n",
    "\n",
    "$$\n",
    "P(D|Q) = \\frac{P(Q|D) \\cdot P(D)}{P(Q)}\n",
    "$$\n",
    "\n",
    "- $P(Q|D)$: Likelihood of the query given the document’s language model (as in the QLM).\n",
    "- $P(D)$: Prior probability of the document (can be uniform or weighted).\n",
    "- $P(Q)$: Normalizing constant (same for all documents and often ignored in ranking).\n",
    "\n",
    "#### Key Characteristics\n",
    "- Puts more emphasis on the document’s prior probability, which can be useful in certain applications like **recommendation systems**.\n",
    "- Less commonly used in traditional ad hoc retrieval tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Divergence Model\n",
    "\n",
    "The **Divergence Model** focuses on the **difference** between the query and document language models. Instead of modeling the likelihood directly, it ranks documents based on how similar or dissimilar their language models are to the query’s language model.\n",
    "\n",
    "#### Core Idea\n",
    "Rank documents based on the divergence (distance) between the query language model $\\theta_Q$ and the document language model $\\theta_D$. This is typically done using **Kullback-Leibler Divergence (KL Divergence)**.\n",
    "\n",
    "#### Mathematical Representation\n",
    "The **KL Divergence** between $\\theta_Q$ and $\\theta_D$ is given by:\n",
    "\n",
    "$$\n",
    "D_{\\text{KL}}(\\theta_Q || \\theta_D) = \\sum_{w \\in V} P(w|Q) \\log \\frac{P(w|Q)}{P(w|D)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $V$: Vocabulary of terms.\n",
    "- $P(w|Q)$: Probability of term $w$ in the query language model.\n",
    "- $P(w|D)$: Probability of term $w$ in the document language model.\n",
    "\n",
    "#### Ranking Criterion\n",
    "Since smaller divergence means greater similarity, documents are ranked by minimizing the divergence:\n",
    "\n",
    "$$\n",
    "\\text{Score}(D, Q) = -D_{\\text{KL}}(\\theta_Q || \\theta_D)\n",
    "$$\n",
    "\n",
    "#### Estimation of $P(w|Q)$ and $P(w|D)$\n",
    "- $P(w|Q)$: Often approximated as the relative frequency of $w$ in the query.\n",
    "- $P(w|D)$: Estimated using **MLE** or smoothing techniques.\n",
    "\n",
    "#### Advantages\n",
    "- Incorporates the entire vocabulary $V$, allowing for a more nuanced comparison of models.\n",
    "- Can capture fine-grained differences between the query and document.\n",
    "\n",
    "#### Limitations\n",
    "- Requires good smoothing techniques to handle rare or unseen terms.\n",
    "- Computationally more expensive than QLM or DLM.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "| Model                    | Ranking Criterion               | Key Concept                                     | Common Use Case                              |\n",
    "|--------------------------|----------------------------------|------------------------------------------------|---------------------------------------------|\n",
    "| **Query Likelihood Model** | $P(Q|D)$                       | Rank documents by how likely they are to generate the query. | Ad hoc information retrieval tasks.         |\n",
    "| **Document Likelihood Model** | $P(D|Q)$                       | Rank documents by their posterior probability given a query. | Recommendation systems, specialized ranking. |\n",
    "| **Divergence Model**      | $-D_{\\text{KL}}(\\theta_Q || \\theta_D)$ | Rank documents by minimizing divergence between query and document language models. | Complex comparisons of models.      |\n",
    "\n",
    "### Premise of Smoothing in Language Models\n",
    "\n",
    "Smoothing in language models addresses the problem of **data sparsity**. The core idea is to adjust probability estimates so that rare or unseen events (e.g., words or word sequences) receive a small, non-zero probability. This avoids assigning zero probability to valid but unseen combinations of words, improving the generalization of the model.\n",
    "\n",
    "---\n",
    "\n",
    "### Why is Smoothing Necessary?\n",
    "\n",
    "Language models estimate the probability of words or sequences of words from a corpus. Without smoothing, any word or sequence not seen in the training data will have a probability of **zero**. This can cause problems such as:\n",
    "\n",
    "1. **Zero Probability for Valid Events:**\n",
    "   - For unseen words or bigrams/trigrams, the language model cannot generate or predict them.\n",
    "   - For example, if \"machine learning\" never appears together in the training data, a bigram model will assign $P(\\text{\"learning\"}|\\text{\"machine\"}) = 0$, which is unrealistic.\n",
    "\n",
    "2. **Poor Generalization:**\n",
    "   - Language models should generalize beyond the training data, but without smoothing, they rely too heavily on observed frequencies and fail to account for rare events.\n",
    "\n",
    "---\n",
    "\n",
    "### How Does Smoothing Work?\n",
    "\n",
    "Smoothing redistributes some probability mass from frequent (or observed) events to rare (or unobserved) events. This ensures that:\n",
    "- No event has zero probability.\n",
    "- The overall probability distribution remains valid (i.e., sums to 1).\n",
    "\n",
    "---\n",
    "\n",
    "### Common Smoothing Techniques\n",
    "\n",
    "#### 1. Laplace Smoothing (Add-One Smoothing)\n",
    "\n",
    "- Adds a constant (usually 1) to the count of each word or n-gram to ensure no probability is zero.\n",
    "- Formula for unigram:\n",
    "  $$\n",
    "  P(w) = \\frac{\\text{Count}(w) + 1}{|C| + |V|}\n",
    "  $$\n",
    "  Where:\n",
    "  - $|C|$: Total count of all words in the corpus.\n",
    "  - $|V|$: Vocabulary size.\n",
    "\n",
    "- **Advantages:**\n",
    "  - Simple and effective for small vocabularies.\n",
    "- **Disadvantages:**\n",
    "  - Overestimates probabilities for rare or unseen events.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Additive (Add-$\\alpha$) Smoothing\n",
    "\n",
    "- A generalization of Laplace smoothing where a smaller constant $\\alpha$ (e.g., 0.1) is added to each count:\n",
    "  $$\n",
    "  P(w) = \\frac{\\text{Count}(w) + \\alpha}{|C| + \\alpha |V|}\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Jelinek-Mercer Smoothing (Interpolation)\n",
    "\n",
    "- Combines higher-order and lower-order models (e.g., bigram + unigram) with a weighted interpolation:\n",
    "  $$\n",
    "  P(w_i|w_{i-1}) = \\lambda P_{\\text{MLE}}(w_i|w_{i-1}) + (1 - \\lambda) P(w_i)\n",
    "  $$\n",
    "  Where $\\lambda$ is a weight parameter.\n",
    "\n",
    "- **Advantages:**\n",
    "  - Balances observed data and a fallback model (e.g., unigram probabilities).\n",
    "- **Disadvantages:**\n",
    "  - Requires tuning of $\\lambda$.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Dirichlet Prior Smoothing\n",
    "\n",
    "- Assumes a Dirichlet prior over the word probabilities and smooths them using a background (corpus-wide) model:\n",
    "  $$\n",
    "  P(w|D) = \\frac{\\text{Count}(w, D) + \\mu P(w|C)}{|D| + \\mu}\n",
    "  $$\n",
    "  Where:\n",
    "  - $\\mu$: Smoothing parameter that controls the influence of the background model.\n",
    "  - $P(w|C)$: Probability of $w$ in the entire collection.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. Kneser-Ney Smoothing\n",
    "\n",
    "- Redistributes probability mass for higher-order n-grams to lower-order ones, focusing on the diversity of word contexts:\n",
    "  $$\n",
    "  P_{\\text{KN}}(w_i|w_{i-1}) = \\max(\\text{Count}(w_{i-1}, w_i) - d, 0) + \\lambda P(w_i)\n",
    "  $$\n",
    "  Where $d$ is a discounting constant.\n",
    "\n",
    "- **Advantages:**\n",
    "  - State-of-the-art for n-gram models.\n",
    "- **Disadvantages:**\n",
    "  - Computationally intensive.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Principles of Smoothing\n",
    "\n",
    "1. **Probability Redistribution:**\n",
    "   - Reduce probabilities of frequent events slightly to assign non-zero probabilities to unseen events.\n",
    "2. **Balance Between Observed and Unseen Data:**\n",
    "   - Combine observed counts with general background knowledge of language.\n",
    "3. **Valid Probability Distribution:**\n",
    "   - Ensure that the total probability across all events remains equal to 1.\n",
    "\n",
    "---\n",
    "\n",
    "### Applications of Smoothing\n",
    "\n",
    "- **Language Modeling:** For tasks like speech recognition, machine translation, and text generation.\n",
    "- **Information Retrieval:** In models like Query Likelihood and Divergence-based retrieval.\n",
    "- **Text Classification:** Where unseen n-grams in test data can appear frequently.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc87e8e7",
   "metadata": {},
   "source": [
    "# Week 3: Information Retrieval Evaluation: \n",
    "\n",
    "What you can't measure you can't improve.\n",
    "\n",
    "Most information retrieval systems are tuned to optimise for an objective evaluation metric. \n",
    "\n",
    "**Measures of IR System**; \n",
    "\n",
    "- *How fast does it index?* Num documents\\hour. Average document size \n",
    "- *How fast does it search?* Latency as a function of index size\n",
    "- *Expressiveness of query language:*  Ability to express complex information needs. Speed on complex queries.\n",
    "\n",
    "Two different approaches to evaluation, **Online and Offline**: \n",
    "\n",
    "**Online**: “Online evaluation is evaluation of a fully functioning system based on implicit\n",
    "measurement of real users’ experiences of the system in a natural usage environment\"\n",
    "\n",
    "**Offline**: Offline metrics are measured in an isolated environment before deploying a new IR\n",
    "system. These look at whether a particular set of relevant results are returned when\n",
    "retrieving items with the system.\n",
    "\n",
    "|            | Online                                      | Offline                                      |\n",
    "|------------|---------------------------------------------|----------------------------------------------|\n",
    "| **Pros**   | Cheap                                       | Fast                                         |\n",
    "|            | Measure actual user reactions              | Easy to try new ideas                       |\n",
    "|            |                                             | Amortized Cost Portable                     |\n",
    "| **Cons**   | Need to go live                             | Needs ground truth                          |\n",
    "|            | Noisy                                       | Slow at the beginning                       |\n",
    "|            | Slow                                        | \"Expensive\"                                 |\n",
    "|            | Not duplicable                             | Inconsistent                                |\n",
    "|            |                                             | Difficult to model how users behave         |\n",
    "\n",
    "- Relevant metrics to determine how good the IR system is at retrieving relevant documents include; \n",
    "\n",
    "**Precision**: The fraction of retrieved documents that are relevant = tp / (tp + fp) \n",
    "**Recall**: The fraction of relevant documents that are retrieved = tp / (tp + fn)\n",
    "**Accuracy**: fraction of documents that are relevant and retrieved or not-relevant and not retrieved divided by total number of documents = (tp + fn) / (tp + fp + fn + tn). This is not always useful, especially when there is a class imbalance between relevant documents and non-relevant documents. Accuracy can still be very high while your IR system is sub-optimal. Better to look at precision and recall instead. \n",
    "**F-Measure**:\n",
    "\n",
    "# **Mean Average Precision (MAP) in Information Retrieval**\n",
    "\n",
    "## **1. Understanding MAP**\n",
    "**Mean Average Precision (MAP)** is a common evaluation metric for **ranking-based information retrieval systems** (e.g., search engines, recommendation systems). It measures the quality of ranked retrieval results by evaluating **both precision and ranking order**.\n",
    "\n",
    "### **Key Components:**\n",
    "- **Precision @ k**: The proportion of relevant documents retrieved in the top-𝑘 results.\n",
    "- **Average Precision (AP)**: The mean of precision values computed at the ranks where relevant documents appear.\n",
    "- **Mean Average Precision (MAP)**: The mean of AP scores across multiple queries.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Formula for MAP**\n",
    "Given a set of **queries** $ Q $, we compute **MAP** as:\n",
    "\n",
    "$$\n",
    "MAP = \\frac{1}{|Q|} \\sum_{q=1}^{|Q|} AP(q)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "AP(q) = \\frac{1}{R} \\sum_{k=1}^{N} P(k) \\cdot rel(k)\n",
    "$$\n",
    "\n",
    "- $ R $ = Total number of relevant documents for query $ q $\n",
    "- $ P(k) $ = Precision at position $ k $\n",
    "- $ rel(k) $ = Binary relevance (1 if the document is relevant, 0 otherwise)\n",
    "- $ N $ = Total retrieved documents\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Example Calculation**\n",
    "### **Scenario**\n",
    "A system retrieves **5 documents** for a query, and the **ground truth** has 3 relevant documents.\n",
    "\n",
    "### **Ranked Retrieval Results:**\n",
    "| Rank | Document | Relevant? (1=Yes, 0=No) | Precision @ k |\n",
    "|------|---------|-------------------------|--------------|\n",
    "| 1    | D1      | ✅ (1)                    | 1.0          |\n",
    "| 2    | D2      | ❌ (0)                    | 0.5          |\n",
    "| 3    | D3      | ✅ (1)                    | 0.67         |\n",
    "| 4    | D4      | ✅ (1)                    | 0.75         |\n",
    "| 5    | D5      | ❌ (0)                    | 0.6          |\n",
    "\n",
    "### **Computing Average Precision (AP)**:\n",
    "\\[\n",
    "AP = \\frac{(1.0 + 0.67 + 0.75)}{3} = \\frac{2.42}{3} = 0.807\n",
    "\\]\n",
    "\n",
    "For multiple queries, we compute **AP** for each and take their mean to get **MAP**.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. MAP vs Other Metrics**\n",
    "| Metric  | Description |\n",
    "|---------|------------|\n",
    "| **Precision@k** | Measures relevant results in top \\( k \\), but ignores ranking order. |\n",
    "| **Recall** | Measures retrieved relevant results but doesn't consider ranking. |\n",
    "| **F1-score** | Balances precision and recall. |\n",
    "| **MAP** | Evaluates ranking by averaging precision across relevant documents. |\n",
    "\n",
    "MAP is **better** than simple **Precision** or **Recall** because it considers the **order of retrieved results**, favoring systems that return **relevant documents earlier**.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. When to Use MAP**\n",
    "✅ **Best suited for:**  \n",
    "- Search engines, recommendation systems, and ranking problems.  \n",
    "- Scenarios where **retrieving all relevant documents is important**.  \n",
    "- Evaluating **ranking quality** over multiple queries.\n",
    "\n",
    "❌ **Not ideal when:**  \n",
    "- You only care about the **top-1 or top-k** results (use **Precision@k** instead).  \n",
    "- A binary decision (relevant/not relevant) is not enough, and graded relevance (e.g., **NDCG**) is required.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088616f8",
   "metadata": {},
   "source": [
    " # Week 5: Intro to ML and Data Mining\n",
    " \n",
    " \n",
    " \n",
    "**Data Mining:** The process of discovering (mining) useful patterns from or conducting inferences based on various types of data sources such as structured information repositories (e.g. databases), text, images, sound, video and so on... \n",
    "\n",
    "Distinction between ML and data mining becoming increasingly difficult. \n",
    "\n",
    "**Association Rule Mining:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d006f9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Bread': 0.6666666666666666, 'Milk': 0.8333333333333334, 'Diaper': 0.5, 'Beer': 0.5, 'Butter': 0.6666666666666666}\n"
     ]
    }
   ],
   "source": [
    "# Mock dataset: Transactions stored in a dictionary\n",
    "transactions = {\n",
    "    1: [\"Bread\", \"Milk\", \"Butter\"],\n",
    "    2: [\"Bread\", \"Butter\"],\n",
    "    3: [\"Milk\", \"Diaper\", \"Beer\"],\n",
    "    4: [\"Bread\", \"Milk\", \"Diaper\", \"Butter\"],\n",
    "    5: [\"Milk\", \"Diaper\", \"Beer\", \"Butter\"],\n",
    "    6: [\"Bread\", \"Milk\", \"Beer\"]\n",
    "}\n",
    "\n",
    "\n",
    "#Finding Support for each item.\n",
    "#Support(X) = Transactions containing X / Total Transactions \n",
    "\n",
    "def build_support(dic, words):\n",
    "\n",
    "    support_dic = {}\n",
    "    \n",
    "    total_transactions = len(dic)\n",
    "    \n",
    "    count = []\n",
    "    \n",
    "    counter = 0\n",
    "    for word in words: \n",
    "        num = 0\n",
    "        for vals in dic.values(): \n",
    "            for val in vals: \n",
    "                if val == word: \n",
    "                    num += 1\n",
    "                else: \n",
    "                    pass\n",
    "        count.append(num)\n",
    "        \n",
    "        support_dic[word] = count[counter] / total_transactions\n",
    "        counter += 1\n",
    "         \n",
    "            \n",
    "    return support_dic\n",
    "\n",
    "\n",
    "unique_words = set()\n",
    "for word in transactions.values():\n",
    "    unique_words.update(word)\n",
    "\n",
    "\n",
    "words = list(unique_words)\n",
    "print(build_support(transactions, words = words))\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c9f757",
   "metadata": {},
   "source": [
    "### The next step in the Apriori Algorithm is to generate frequent itemsets and prune infrequent ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4010ee9a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frequent_items: {'Bread': 0.6666666666666666, 'Milk': 0.8333333333333334, 'Diaper': 0.5, 'Beer': 0.5, 'Butter': 0.6666666666666666}\n",
      "L2 Candidates: [('Bread', 'Milk'), ('Bread', 'Diaper'), ('Bread', 'Beer'), ('Bread', 'Butter'), ('Milk', 'Diaper'), ('Milk', 'Beer'), ('Milk', 'Butter'), ('Diaper', 'Beer'), ('Diaper', 'Butter'), ('Beer', 'Butter')]\n",
      "L3 Candidates: [('Bread', 'Diaper', 'Milk'), ('Beer', 'Bread', 'Milk'), ('Bread', 'Butter', 'Milk'), ('Beer', 'Bread', 'Diaper'), ('Bread', 'Butter', 'Diaper'), ('Beer', 'Bread', 'Butter'), ('Beer', 'Diaper', 'Milk'), ('Butter', 'Diaper', 'Milk'), ('Beer', 'Butter', 'Milk'), ('Beer', 'Butter', 'Diaper')]\n"
     ]
    }
   ],
   "source": [
    "support_dic = build_support(transactions, words = words)\n",
    "\n",
    "min_support = 0.5 # must define minimum support: \n",
    "\n",
    "frequent_items = {}\n",
    "\n",
    "for key, value in support_dic.items():\n",
    "    if value >= min_support: \n",
    "        frequent_items[key] = value\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "  \n",
    "print(\"frequent_items:\", frequent_items)\n",
    "# Now want to generate \n",
    "\n",
    "#Generate candidate itemsets of size k from frequent (k-1)-itemsets\n",
    "\n",
    "def generate_candidates(frequent_itemsets, k):\n",
    "    \"\"\"Generate candidate itemsets of size k from frequent (k-1)-itemsets.\"\"\"\n",
    "    itemsets = list(frequent_itemsets.keys())  # Convert dictionary keys to a list\n",
    "    candidates = []  # Store candidate itemsets\n",
    "\n",
    "    for i in range(len(itemsets)):\n",
    "        for j in range(i + 1, len(itemsets)):  \n",
    "            itemset1 = itemsets[i]\n",
    "            itemset2 = itemsets[j]\n",
    "\n",
    "\n",
    "            if k == 2:  # Directly form 2-itemsets\n",
    "                candidates.append((itemset1, itemset2))\n",
    "            else:  \n",
    "                # Check if first (k-1) elements are the same (prefix matching)\n",
    "                if itemset1[:-1] == itemset2[:-1]:  \n",
    "                    candidate = tuple(sorted(set(itemset1) | set(itemset2)))  # Merge and sort\n",
    "\n",
    "                    if len(candidate) == k and candidate not in candidates:\n",
    "                        candidates.append(candidate)\n",
    "\n",
    "    return candidates\n",
    "\n",
    "# Generate candidate 2-itemsets\n",
    "L2_candidates = generate_candidates(frequent_items, k=2)\n",
    "print(\"L2 Candidates:\", L2_candidates)\n",
    "\n",
    "# Generate candidate 3-itemsets using L2\n",
    "L3_candidates = generate_candidates({itemset: 0 for itemset in L2_candidates}, k=3)\n",
    "print(\"L3 Candidates:\", L3_candidates)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "423572ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('Bread', 'Milk'): 0.5, ('Bread', 'Diaper'): 0.16666666666666666, ('Bread', 'Beer'): 0.16666666666666666, ('Bread', 'Butter'): 0.5, ('Milk', 'Diaper'): 0.5, ('Milk', 'Beer'): 0.5, ('Milk', 'Butter'): 0.5, ('Diaper', 'Beer'): 0.3333333333333333, ('Diaper', 'Butter'): 0.3333333333333333, ('Beer', 'Butter'): 0.16666666666666666}\n"
     ]
    }
   ],
   "source": [
    "transactions = {\n",
    "    1: [\"Bread\", \"Milk\", \"Butter\"],\n",
    "    2: [\"Bread\", \"Butter\"],\n",
    "    3: [\"Milk\", \"Diaper\", \"Beer\"],\n",
    "    4: [\"Bread\", \"Milk\", \"Diaper\", \"Butter\"],\n",
    "    5: [\"Milk\", \"Diaper\", \"Beer\", \"Butter\"],\n",
    "    6: [\"Bread\", \"Milk\", \"Beer\"]\n",
    "}\n",
    "\n",
    "\n",
    "candidates = generate_candidates(frequent_items, k=2)\n",
    "#print(candidates)\n",
    "\n",
    "def count_support(transactions, candidates):\n",
    "    \n",
    "    support_counts = {}\n",
    "    for itemset in candidates: \n",
    "        support_counts[itemset] = 0\n",
    "        \n",
    "    for transaction in transactions.values(): \n",
    "        transaction_set = set(transaction)\n",
    "        for candidate in candidates:\n",
    "            if set(candidate).issubset(transaction_set): \n",
    "                support_counts[candidate] += 1\n",
    "    \n",
    "    total_transactions = len(transactions)\n",
    "    support_values = {}\n",
    "    \n",
    "    for candidate in candidates: \n",
    "        support_values[candidate] = support_counts[candidate]/total_transactions\n",
    "    \n",
    "    return support_values\n",
    "   \n",
    "\n",
    "print(count_support(transactions = transactions, candidates = candidates))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "df09b8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 Candidates: [('Bread', 'Milk'), ('Bread', 'Diaper'), ('Bread', 'Beer'), ('Bread', 'Butter'), ('Milk', 'Diaper'), ('Milk', 'Beer'), ('Milk', 'Butter'), ('Diaper', 'Beer'), ('Diaper', 'Butter'), ('Beer', 'Butter')]\n",
      "\n",
      "L2 Support: {('Bread', 'Milk'): 0.5, ('Bread', 'Diaper'): 0.16666666666666666, ('Bread', 'Beer'): 0.16666666666666666, ('Bread', 'Butter'): 0.5, ('Milk', 'Diaper'): 0.5, ('Milk', 'Beer'): 0.5, ('Milk', 'Butter'): 0.5, ('Diaper', 'Beer'): 0.3333333333333333, ('Diaper', 'Butter'): 0.3333333333333333, ('Beer', 'Butter'): 0.16666666666666666}\n"
     ]
    }
   ],
   "source": [
    "L2_candidates = generate_candidates(frequent_items, k=2)\n",
    "print(\"L2 Candidates:\", L2_candidates)\n",
    "print()\n",
    "L2_support = count_support(transactions, L2_candidates)\n",
    "print(\"L2 Support:\", L2_support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "04c9d2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('Bread', 'Milk'): 0.5, ('Bread', 'Diaper'): 0.16666666666666666, ('Bread', 'Beer'): 0.16666666666666666, ('Bread', 'Butter'): 0.5, ('Milk', 'Diaper'): 0.5, ('Milk', 'Beer'): 0.5, ('Milk', 'Butter'): 0.5, ('Diaper', 'Beer'): 0.3333333333333333, ('Diaper', 'Butter'): 0.3333333333333333, ('Beer', 'Butter'): 0.16666666666666666}\n",
      "\n",
      "Frequent L2 Itemsets: {('Bread', 'Milk'): 0.5, ('Bread', 'Butter'): 0.5, ('Milk', 'Diaper'): 0.5, ('Milk', 'Beer'): 0.5, ('Milk', 'Butter'): 0.5}\n"
     ]
    }
   ],
   "source": [
    "def prune_candidates(support_values, min_support): \n",
    "    \n",
    "    pruned_support_values = {}\n",
    "    \n",
    "    for candidate, support in support_values.items(): \n",
    "        if support >= min_support: \n",
    "            pruned_support_values[candidate] = support\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    return pruned_support_values\n",
    "\n",
    "print(L2_support)\n",
    "print()\n",
    "\n",
    "L2_frequent = prune_candidates(L2_support, min_support)\n",
    "print(\"Frequent L2 Itemsets:\", L2_frequent)            \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa3cd00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "be944ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequent L3 Itemsets: {}\n"
     ]
    }
   ],
   "source": [
    "L3_candidates = generate_candidates(L2_frequent, k=3)\n",
    "L3_support = count_support(transactions, L3_candidates)\n",
    "L3_frequent = prune_candidates(L3_support, min_support)\n",
    "print(\"Frequent L3 Itemsets:\", L3_frequent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcea088e",
   "metadata": {},
   "source": [
    "## Step 4️⃣: Generate Association Rules\n",
    "\n",
    "Once we have frequent itemsets, we generate association rules of the form:\n",
    "\n",
    "A $\\rightarrow$ B\n",
    "\n",
    "where:\n",
    "\n",
    "\t•\tConfidence measures how often B appears when A is present.\n",
    "\t•\tLift compares confidence to expected frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82aba25f",
   "metadata": {},
   "source": [
    "\n",
    "### **🔹 1️⃣ Confidence Formula**\n",
    "Confidence measures the **likelihood** of seeing $B$ given that $A$ has occurred.\n",
    "\n",
    "$$\n",
    "\\text{Confidence}(A \\rightarrow B) = \\frac{\\text{Support}(A \\cup B)}{\\text{Support}(A)}\n",
    "$$\n",
    "\n",
    "**Interpretation**:  \n",
    "- $\\text{Confidence}(A \\rightarrow B) = 80\\%$ means that when $A$ appears, $B$ also appears **80% of the time** in the dataset.\n",
    "- Higher confidence suggests a **stronger** association.\n",
    "\n",
    "✅ **Example Calculation**:  \n",
    "- $\\text{Support}(\\{Milk, Bread\\}) = 0.4$  \n",
    "- $\\text{Support}(\\{Milk\\}) = 0.5$  \n",
    "- $\\text{Confidence}(\\{Milk\\} \\rightarrow \\{Bread\\})$:\n",
    "\n",
    "$$\n",
    "\\frac{0.4}{0.5} = 0.8 \\quad (80\\%)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **🔹 2️⃣ Lift Formula**\n",
    "Lift measures how much **more likely** $B$ appears **when A is present**, compared to $B$ appearing randomly.\n",
    "\n",
    "$$\n",
    "\\text{Lift}(A \\rightarrow B) = \\frac{\\text{Confidence}(A \\rightarrow B)}{\\text{Support}(B)}\n",
    "$$\n",
    "\n",
    "Or equivalently:\n",
    "\n",
    "$$\n",
    "\\text{Lift}(A \\rightarrow B) = \\frac{\\text{Support}(A \\cup B)}{\\text{Support}(A) \\times \\text{Support}(B)}\n",
    "$$\n",
    "\n",
    "**Interpretation**:\n",
    "- **Lift = 1** → $A$ and $B$ are **independent** (no association).\n",
    "- **Lift > 1** → $A$ and $B$ occur **more often together than expected** (positive association).\n",
    "- **Lift < 1** → $A$ actually **reduces** the likelihood of $B$ occurring (negative association).\n",
    "\n",
    "✅ **Example Calculation**:  \n",
    "- $\\text{Confidence}(\\{Milk\\} \\rightarrow \\{Bread\\}) = 0.8$\n",
    "- $\\text{Support}(\\{Bread\\}) = 0.6$\n",
    "- $\\text{Lift}(\\{Milk\\} \\rightarrow \\{Bread\\})$:\n",
    "\n",
    "$$\n",
    "\\frac{0.8}{0.6} = 1.33\n",
    "$$\n",
    "\n",
    "This means that **buying Milk increases the likelihood of buying Bread by 1.33 times compared to random chance**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "af8d6634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Itemset: ('Bread', 'Milk')\n",
      "Antecedent: ('Bread',), Consequent: ('Milk',)\n",
      "Support(A ∪ B) = 0.5, Support(A) = 0.6666666666666666\n",
      "Computed Confidence = 0.75\n",
      "Antecedent: ('Milk',), Consequent: ('Bread',)\n",
      "Support(A ∪ B) = 0.5, Support(A) = 0.8333333333333334\n",
      "Computed Confidence = 0.6\n",
      "\n",
      "Processing Itemset: ('Bread', 'Butter')\n",
      "Antecedent: ('Bread',), Consequent: ('Butter',)\n",
      "Support(A ∪ B) = 0.5, Support(A) = 0.6666666666666666\n",
      "Computed Confidence = 0.75\n",
      "Antecedent: ('Butter',), Consequent: ('Bread',)\n",
      "Support(A ∪ B) = 0.5, Support(A) = 0.6666666666666666\n",
      "Computed Confidence = 0.75\n",
      "\n",
      "Processing Itemset: ('Milk', 'Diaper')\n",
      "Antecedent: ('Milk',), Consequent: ('Diaper',)\n",
      "Support(A ∪ B) = 0.5, Support(A) = 0.8333333333333334\n",
      "Computed Confidence = 0.6\n",
      "Antecedent: ('Diaper',), Consequent: ('Milk',)\n",
      "Support(A ∪ B) = 0.5, Support(A) = 0.5\n",
      "Computed Confidence = 1.0\n",
      "\n",
      "Processing Itemset: ('Milk', 'Beer')\n",
      "Antecedent: ('Milk',), Consequent: ('Beer',)\n",
      "Support(A ∪ B) = 0.5, Support(A) = 0.8333333333333334\n",
      "Computed Confidence = 0.6\n",
      "Antecedent: ('Beer',), Consequent: ('Milk',)\n",
      "Support(A ∪ B) = 0.5, Support(A) = 0.5\n",
      "Computed Confidence = 1.0\n",
      "\n",
      "Processing Itemset: ('Milk', 'Butter')\n",
      "Antecedent: ('Milk',), Consequent: ('Butter',)\n",
      "Support(A ∪ B) = 0.5, Support(A) = 0.8333333333333334\n",
      "Computed Confidence = 0.6\n",
      "Antecedent: ('Butter',), Consequent: ('Milk',)\n",
      "Support(A ∪ B) = 0.5, Support(A) = 0.6666666666666666\n",
      "Computed Confidence = 0.75\n",
      "Association Rules: [(('Bread',), ('Milk',), 0.75), (('Milk',), ('Bread',), 0.6), (('Bread',), ('Butter',), 0.75), (('Butter',), ('Bread',), 0.75), (('Milk',), ('Diaper',), 0.6), (('Diaper',), ('Milk',), 1.0), (('Milk',), ('Beer',), 0.6), (('Beer',), ('Milk',), 1.0), (('Milk',), ('Butter',), 0.6), (('Butter',), ('Milk',), 0.75)]\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "def generate_rules(frequent_itemsets, support_values, L1_support, min_confidence):\n",
    "    \"\"\"Generates strong association rules from frequent itemsets.\"\"\"\n",
    "    rules = []\n",
    "\n",
    "    for itemset in frequent_itemsets.keys():\n",
    "        length = len(itemset)\n",
    "        if length < 2:\n",
    "            continue  # Skip single items\n",
    "\n",
    "        print(f\"\\nProcessing Itemset: {itemset}\")  # Debug print\n",
    "\n",
    "        # Generate all possible subsets of the itemset\n",
    "        for i in range(1, length):\n",
    "            subsets = list(combinations(itemset, i))\n",
    "\n",
    "            for antecedent in subsets:\n",
    "                consequent = tuple(sorted(set(itemset) - set(antecedent)))  # Find remaining items\n",
    "                if not consequent:  # Skip empty consequents\n",
    "                    continue\n",
    "\n",
    "                # Use L1_support for single-item antecedents\n",
    "                antecedent_support = support_values.get(antecedent, L1_support.get(antecedent, None))\n",
    "\n",
    "                # Debug: Print support values\n",
    "                print(f\"Antecedent: {antecedent}, Consequent: {consequent}\")\n",
    "                print(f\"Support(A ∪ B) = {support_values.get(itemset, 'Missing')}, Support(A) = {antecedent_support}\")\n",
    "\n",
    "                if antecedent_support:\n",
    "                    confidence = support_values[itemset] / antecedent_support\n",
    "                    print(f\"Computed Confidence = {confidence}\")\n",
    "\n",
    "                    if confidence >= min_confidence:\n",
    "                        rules.append((antecedent, consequent, confidence))\n",
    "\n",
    "    return rules\n",
    "\n",
    "min_confidence = 0  # Define minimum confidence threshold\n",
    "L1_support = count_support(transactions, [(item,) for item in frequent_items.keys()])\n",
    "\n",
    "association_rules = generate_rules(L2_frequent, L2_support, L1_support, min_confidence)\n",
    "print(\"Association Rules:\", association_rules)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64728ff",
   "metadata": {},
   "source": [
    "### **Apriori Algorithm: Association Rule Mining**\n",
    "\n",
    "The **Apriori algorithm** is used to identify **frequent itemsets** and generate **association rules** in transaction datasets. It follows these steps:\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 1️⃣: Compute Support for Single Items (L1)**\n",
    "\n",
    "- Support of an itemset $X$ is defined as:\n",
    "\n",
    "$$\n",
    "\\text{Support}(X) = \\frac{\\text{Transactions containing } X}{\\text{Total Transactions}}\n",
    "$$\n",
    "\n",
    "- Identify **frequent 1-itemsets** that satisfy the **minimum support threshold** $\\text{min\\_support}$.\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 2️⃣: Generate Candidate Itemsets (L2, L3, …)**\n",
    "\n",
    "- Using **frequent $(k-1)$-itemsets**, generate **$k$-item candidates**.\n",
    "- The **Apriori property** states that any **subset** of a frequent itemset **must also be frequent**.\n",
    "\n",
    "### **Candidate Generation:**\n",
    "For itemsets of size $k$:\n",
    "- If two itemsets of size $(k-1)$ share the first $(k-2)$ items, merge them into a $k$-itemset.\n",
    "- Ensure the new candidate contains **no duplicate elements**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 3️⃣: Prune Candidates Below `min_support`**\n",
    "- For each candidate itemset, count its occurrence in transactions.\n",
    "- Remove candidates that do not satisfy:\n",
    "\n",
    "$$\n",
    "\\text{Support}(X) \\geq \\text{min\\_support}\n",
    "$$\n",
    "\n",
    "- The remaining itemsets are **frequent itemsets**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 4️⃣: Repeat for Larger Itemsets (`L3`, `L4`, …)**\n",
    "- Use the **frequent L2 itemsets** to generate **L3 candidates**.\n",
    "- Compute **support** and **prune**.\n",
    "- Repeat until **no new frequent itemsets can be formed**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 5️⃣: Generate Association Rules**\n",
    "Once frequent itemsets are identified, generate **association rules**:\n",
    "\n",
    "$$\n",
    "A \\rightarrow B\n",
    "$$\n",
    "\n",
    "where:\n",
    "- **$A$ (Antecedent):** Left-hand side of the rule.\n",
    "- **$B$ (Consequent):** Right-hand side of the rule.\n",
    "\n",
    "Rules are evaluated using:\n",
    "\n",
    "### **1️⃣ Confidence**\n",
    "Confidence measures the probability of observing $B$ given $A$:\n",
    "\n",
    "$$\n",
    "\\text{Confidence}(A \\rightarrow B) = \\frac{\\text{Support}(A \\cup B)}{\\text{Support}(A)}\n",
    "$$\n",
    "\n",
    "A high confidence value means that when $A$ appears, $B$ is also likely to appear.\n",
    "\n",
    "---\n",
    "\n",
    "### **2️⃣ Lift**\n",
    "Lift measures how much **more likely** $B$ appears **when A is present**, compared to $B$ appearing randomly.\n",
    "\n",
    "$$\n",
    "\\text{Lift}(A \\rightarrow B) = \\frac{\\text{Support}(A \\cup B)}{\\text{Support}(A) \\times \\text{Support}(B)}\n",
    "$$\n",
    "\n",
    "#### **Interpretation:**\n",
    "- **Lift = 1** → $A$ and $B$ are **independent** (no association).\n",
    "- **Lift > 1** → $A$ and $B$ occur **more often together than expected** (**positive association**).\n",
    "- **Lift < 1** → $A$ actually **reduces** the likelihood of $B$ occurring (**negative association**).\n",
    "\n",
    "---\n",
    "\n",
    "## **📌 Summary of the Pipeline**\n",
    "| **Step**  | **What It Does** |\n",
    "|------------|----------------|\n",
    "| **Step 1** | Compute **L1 (single-item) support** and filter based on `min_support` |\n",
    "| **Step 2** | Generate **L2 candidates** using the **Apriori property** |\n",
    "| **Step 3** | Compute **support for L2** and **prune infrequent itemsets** |\n",
    "| **Step 4** | **Repeat for L3, L4, ...** until no more candidates exist |\n",
    "| **Step 5** | Generate **association rules** and compute **confidence & lift** |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aab150b",
   "metadata": {},
   "source": [
    "# Coursework Notes: \n",
    "\n",
    "\n",
    "# **Task 1: Evaluating Retrieval Quality (20 marks)**  \n",
    "\n",
    "This task involves implementing **two ranking metrics**:  \n",
    "1. **Average Precision (AP)**  \n",
    "2. **Normalized Discounted Cumulative Gain (NDCG)**  \n",
    "\n",
    "You will compute these metrics for **BM25 rankings** on the validation set.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Understanding the Metrics**\n",
    "### **1.1. Average Precision (AP)**\n",
    "- Measures the quality of ranked retrieval results.\n",
    "- Formula:\n",
    "\n",
    "$$\n",
    "AP = \\frac{1}{R} \\sum_{k=1}^{N} P(k) \\cdot rel(k)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- \\( R \\) = total number of **relevant** passages.\n",
    "- \\( P(k) \\) = precision at rank \\( k \\).\n",
    "- \\( rel(k) \\) = 1 if the passage at rank \\( k \\) is relevant, else 0.\n",
    "\n",
    "---\n",
    "\n",
    "### **1.2. Normalized Discounted Cumulative Gain (NDCG)**\n",
    "- Measures how well **highly relevant** documents are ranked at the top.\n",
    "- Formula:\n",
    "\n",
    "$$\n",
    "DCG@k = \\sum_{i=1}^{k} \\frac{rel_i}{\\log_2(i+1)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "NDCG@k = \\frac{DCG@k}{IDCG@k}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- \\( rel_i \\) = relevance score at rank \\( i \\).\n",
    "- \\( IDCG@k \\) = ideal (best possible) DCG at **cutoff \\( k \\)**.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Implementing the Metrics**\n",
    "Below is the Python implementation:\n",
    "\n",
    "```python\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def average_precision(relevance_list):\n",
    "    \"\"\"\n",
    "    Compute Average Precision (AP).\n",
    "    \n",
    "    relevance_list: List of binary relevance scores (1 for relevant, 0 for not relevant)\n",
    "    \n",
    "    Returns:\n",
    "        AP score\n",
    "    \"\"\"\n",
    "    num_relevant = sum(relevance_list)\n",
    "    if num_relevant == 0:\n",
    "        return 0.0\n",
    "\n",
    "    precision_sum = 0.0\n",
    "    relevant_count = 0\n",
    "\n",
    "    for i, rel in enumerate(relevance_list):\n",
    "        if rel == 1:\n",
    "            relevant_count += 1\n",
    "            precision_at_i = relevant_count / (i + 1)  # Precision at rank i\n",
    "            precision_sum += precision_at_i\n",
    "\n",
    "    return precision_sum / num_relevant\n",
    "\n",
    "def dcg_at_k(relevance_list, k):\n",
    "    \"\"\"\n",
    "    Compute Discounted Cumulative Gain (DCG) at rank k.\n",
    "    \n",
    "    relevance_list: List of relevance scores (binary or graded)\n",
    "    k: Rank cutoff\n",
    "    \n",
    "    Returns:\n",
    "        DCG@k score\n",
    "    \"\"\"\n",
    "    relevance_list = np.array(relevance_list[:k])\n",
    "    discounts = np.log2(np.arange(2, len(relevance_list) + 2))\n",
    "    return np.sum(relevance_list / discounts)\n",
    "\n",
    "def ndcg_at_k(relevance_list, k):\n",
    "    \"\"\"\n",
    "    Compute Normalized Discounted Cumulative Gain (NDCG) at rank k.\n",
    "    \n",
    "    relevance_list: List of relevance scores (binary or graded)\n",
    "    k: Rank cutoff\n",
    "    \n",
    "    Returns:\n",
    "        NDCG@k score\n",
    "    \"\"\"\n",
    "    dcg_k = dcg_at_k(relevance_list, k)\n",
    "    ideal_relevance_list = sorted(relevance_list, reverse=True)  # Ideal ranking\n",
    "    idcg_k = dcg_at_k(ideal_relevance_list, k)\n",
    "    \n",
    "    return dcg_k / idcg_k if idcg_k > 0 else 0.0\n",
    "\n",
    "# Example usage with dummy relevance list\n",
    "relevance_example = [1, 0, 1, 1, 0, 0, 1]  # Example relevance scores\n",
    "\n",
    "ap_score = average_precision(relevance_example)\n",
    "ndcg_score = ndcg_at_k(relevance_example, k=5)\n",
    "\n",
    "ap_score, ndcg_score\n",
    "\n",
    "```\n",
    "\n",
    "# **Numerical Example for Average Precision (AP) and NDCG**\n",
    "\n",
    "Let's go step by step with a small **ranked list of passages** and their **relevance scores**.\n",
    "\n",
    "## **Example Setup**\n",
    "Suppose we have **5 retrieved passages** for a query, and their **binary relevance scores** (1 for relevant, 0 for non-relevant) are:\n",
    "\n",
    "| Rank | Passage ID | Relevance ($rel$) |\n",
    "|------|-----------|----------------|\n",
    "| 1    | P1        | 1              |\n",
    "| 2    | P2        | 0              |\n",
    "| 3    | P3        | 1              |\n",
    "| 4    | P4        | 1              |\n",
    "| 5    | P5        | 0              |\n",
    "\n",
    "Now, let's compute **AP and NDCG@5** step by step.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Calculating Average Precision (AP)**\n",
    "The formula for Average Precision is:\n",
    "\n",
    "$$\n",
    "AP = \\frac{1}{R} \\sum_{k=1}^{N} P(k) \\cdot rel(k)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $R$ = total number of relevant passages = **3** (P1, P3, P4).\n",
    "- $P(k)$ = precision at rank $k$.\n",
    "- $rel(k)$ = 1 if passage at rank $k$ is relevant, else 0.\n",
    "\n",
    "### **Step-by-step calculation**\n",
    "Compute **precision at each rank** where $rel(k) = 1$:\n",
    "\n",
    "| Rank k | Relevance rel(k) | Precision P(k) | Contribution to AP |\n",
    "|------|------------|----------------|-----------------------------|\n",
    "| 1    | 1          | $ 1/1 = 1.0 $  | $ 1.0 \\times 1 = 1.0 $ |\n",
    "| 2    | 0          | -              | - |\n",
    "| 3    | 1          | $ 2/3 = 0.667 $| $ 0.667 \\times 1 = 0.667 $ |\n",
    "| 4    | 1          | $ 3/4 = 0.75 $ | $ 0.75 \\times 1 = 0.75 $ |\n",
    "| 5    | 0          | -              | - |\n",
    "\n",
    "Now, compute the final **AP**:\n",
    "\n",
    "$$\n",
    "AP = \\frac{(1.0 + 0.667 + 0.75)}{3} = \\frac{2.417}{3} = 0.806\n",
    "$$\n",
    "\n",
    "So, **AP = 0.806**.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Calculating NDCG@5**\n",
    "The formula for **Discounted Cumulative Gain (DCG)** is:\n",
    "\n",
    "$$\n",
    "DCG@k = \\sum_{i=1}^{k} \\frac{rel_i}{\\log_2(i+1)}\n",
    "$$\n",
    "\n",
    "And **Normalized DCG (NDCG)** is:\n",
    "\n",
    "$$\n",
    "NDCG@k = \\frac{DCG@k}{IDCG@k}\n",
    "$$\n",
    "\n",
    "where $IDCG@k$ is the **ideal** DCG (i.e., the best possible ranking).\n",
    "\n",
    "### **Step-by-step calculation**\n",
    "Compute **DCG@5**:\n",
    "\n",
    "$$\n",
    "DCG@5 = \\frac{1}{\\log_2(1+1)} + \\frac{0}{\\log_2(2+1)} + \\frac{1}{\\log_2(3+1)} + \\frac{1}{\\log_2(4+1)} + \\frac{0}{\\log_2(5+1)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{1}{\\log_2(2)} + \\frac{0}{\\log_2(3)} + \\frac{1}{\\log_2(4)} + \\frac{1}{\\log_2(5)} + \\frac{0}{\\log_2(6)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{1}{1} + \\frac{0}{1.585} + \\frac{1}{2} + \\frac{1}{2.322} + \\frac{0}{2.585}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 1 + 0 + 0.5 + 0.43 + 0 = 1.93\n",
    "$$\n",
    "\n",
    "### **Computing Ideal DCG (IDCG@5)**\n",
    "The **ideal** ranking is the one with all relevant passages ranked first:\n",
    "\n",
    "| Rank | Ideal Relevance |\n",
    "|------|----------------|\n",
    "| 1    | 1              |\n",
    "| 2    | 1              |\n",
    "| 3    | 1              |\n",
    "| 4    | 0              |\n",
    "| 5    | 0              |\n",
    "\n",
    "$$\n",
    "IDCG@5 = \\frac{1}{\\log_2(1+1)} + \\frac{1}{\\log_2(2+1)} + \\frac{1}{\\log_2(3+1)} + \\frac{0}{\\log_2(4+1)} + \\frac{0}{\\log_2(5+1)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{1}{1} + \\frac{1}{1.585} + \\frac{1}{2} + \\frac{0}{2.322} + \\frac{0}{2.585}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 1 + 0.63 + 0.5 + 0 + 0 = 2.13\n",
    "$$\n",
    "\n",
    "### **Final NDCG@5 Calculation**\n",
    "$$\n",
    "NDCG@5 = \\frac{DCG@5}{IDCG@5} = \\frac{1.93}{2.13} = 0.906\n",
    "$$\n",
    "\n",
    "So, **NDCG@5 = 0.906**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Final Results**\n",
    "For this example:\n",
    "- **AP = 0.806**\n",
    "- **NDCG@5 = 0.906**\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980e58eb",
   "metadata": {},
   "source": [
    "# Task 2: Logistic Regression\n",
    "\n",
    "## Overview: \n",
    "\n",
    "**Key Steps**\n",
    "1) \tRepresent queries and passages using word embeddings\n",
    "    \n",
    "            •\tUse Word2Vec, GloVe, FastText, or ELMo to obtain embeddings for words.\n",
    "            •\tCompute query and passage embeddings by averaging word embeddings.\n",
    "            \n",
    "2)\tImplement Logistic Regression (from scratch)\n",
    "\n",
    "            •\tDefine a logistic regression function to predict passage relevance.\n",
    "            •\tTrain using gradient descent.\n",
    "            •\tEvaluate using AP and NDCG.\n",
    "            \n",
    "3) Analyze Learning Rate\n",
    "\n",
    "            •\tTrain with different learning rates.\n",
    "            •\tObserve impact on training loss over epochs.\n",
    "            \n",
    "4)\tConsider Negative Sampling (Optional)\n",
    "\t\n",
    "            •\tIf training data is imbalanced, generate additional negative examples.\n",
    "\n",
    "# **Numerical Example: Computing Query & Passage Embeddings**\n",
    "Let's go step by step with a small numerical example to **manually compute query and passage embeddings**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 1: Define a Small Vocabulary with Word Embeddings**\n",
    "Assume we have a small **word embedding dictionary** where each word is represented by a **3-dimensional vector** (instead of 300d for simplicity).\n",
    "\n",
    "| Word        | Embedding (3D) |\n",
    "|------------|---------------------|\n",
    "| machine    | **[0.2, 0.4, 0.6]**  |\n",
    "| learning   | **[0.3, 0.7, 0.5]**  |\n",
    "| deep       | **[0.1, 0.3, 0.9]**  |\n",
    "| subset     | **[0.5, 0.2, 0.1]**  |\n",
    "| techniques | **[0.6, 0.8, 0.4]**  |\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 2: Tokenize the Query and Passage**\n",
    "We have:\n",
    "\n",
    "**Query:** `\"machine learning techniques\"`  \n",
    "**Passage:** `\"deep learning is a subset of machine learning\"`\n",
    "\n",
    "After tokenization and removing stopwords:\n",
    "\n",
    "Query Tokens = [“machine”, “learning”, “techniques”]\n",
    "Passage Tokens = [“deep”, “learning”, “subset”, “machine”, “learning”]\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 3: Retrieve Word Embeddings**\n",
    "Using the table above, we retrieve the embeddings for each word.\n",
    "\n",
    "### **Query Embeddings:**\n",
    "- `machine` → **[0.2, 0.4, 0.6]**\n",
    "- `learning` → **[0.3, 0.7, 0.5]**\n",
    "- `techniques` → **[0.6, 0.8, 0.4]**\n",
    "\n",
    "### **Passage Embeddings:**\n",
    "- `deep` → **[0.1, 0.3, 0.9]**\n",
    "- `learning` → **[0.3, 0.7, 0.5]**\n",
    "- `subset` → **[0.5, 0.2, 0.1]**\n",
    "- `machine` → **[0.2, 0.4, 0.6]**\n",
    "- `learning` → **[0.3, 0.7, 0.5]** (again)\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 4: Compute Mean Embeddings**\n",
    "We compute the **mean vector** for the query and passage by averaging the embeddings.\n",
    "\n",
    "### **Query Embedding:**\n",
    "\n",
    "$$\n",
    "E_q = \\frac{1}{3} \\sum_{i=1}^{3} v_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "E_q = \\frac{1}{3} \\left( [0.2, 0.4, 0.6] + [0.3, 0.7, 0.5] + [0.6, 0.8, 0.4] \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{1}{3} [ (0.2+0.3+0.6), (0.4+0.7+0.8), (0.6+0.5+0.4) ]\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{1}{3} [ 1.1, 1.9, 1.5 ]\n",
    "$$\n",
    "\n",
    "$$\n",
    "= [0.367, 0.633, 0.5]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Passage Embedding:**\n",
    "\n",
    "$$\n",
    "E_p = \\frac{1}{5} \\sum_{i=1}^{5} v_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "E_p = \\frac{1}{5} \\left( [0.1, 0.3, 0.9] + [0.3, 0.7, 0.5] + [0.5, 0.2, 0.1] + [0.2, 0.4, 0.6] + [0.3, 0.7, 0.5] \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{1}{5} [ (0.1+0.3+0.5+0.2+0.3), (0.3+0.7+0.2+0.4+0.7), (0.9+0.5+0.1+0.6+0.5) ]\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{1}{5} [ 1.4, 2.3, 2.6 ]\n",
    "$$\n",
    "\n",
    "$$\n",
    "= [0.28, 0.46, 0.52]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## **Final Output: Embeddings**\n",
    "- **Query Embedding:** **[0.367, 0.633, 0.5]**\n",
    "- **Passage Embedding:** **[0.28, 0.46, 0.52]**\n",
    "\n",
    "These **fixed-length embeddings** now represent the query and passage numerically and will be used as input for **Logistic Regression**.\n",
    "\n",
    "---\n",
    "\n",
    "# **Logistic Regression for Passage Relevance**\n",
    "\n",
    "Now that we have **query and passage embeddings**, we can implement **Logistic Regression** to predict whether a passage is **relevant** to a given query.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Logistic Regression Overview**\n",
    "Logistic Regression is a binary classification model that estimates the probability that a passage is relevant to a query.\n",
    "\n",
    "### **Mathematical Formulation**\n",
    "Given an input feature vector $ x $, the **logistic regression model** predicts a probability $ \\hat{y} $:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\sigma(w^T x + b)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ x $ = **[query embedding ⊕ passage embedding]** (concatenation of query & passage vectors)\n",
    "- $ w $ = **trainable weight vector** (to be learned)\n",
    "- $ b $ = **bias term** (trainable scalar)\n",
    "- $ \\sigma(z) = \\frac{1}{1 + e^{-z}} $ = **sigmoid function**\n",
    "\n",
    "### **Loss Function: Binary Cross-Entropy**\n",
    "Since this is a binary classification task (relevant = 1, not relevant = 0), we optimize the **binary cross-entropy loss**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = - \\frac{1}{N} \\sum_{i=1}^{N} y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ y_i $ = ground truth (1 if relevant, 0 otherwise)\n",
    "- $ \\hat{y}_i $ = predicted probability\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Feature Representation**\n",
    "Each input sample consists of:\n",
    "1. **Query embedding** $ E_q $ (300D)\n",
    "2. **Passage embedding** $ E_p $ (300D)\n",
    "3. **Final input vector**:\n",
    "\n",
    "$$\n",
    "x = [E_q \\oplus E_p]\n",
    "$$\n",
    "\n",
    "This results in a **600-dimensional input vector** ($ x \\in \\mathbb{R}^{600} $).\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Training Logistic Regression**\n",
    "### **Step 1: Initialize Parameters**\n",
    "We initialize the weight vector $ w $ and bias $ b $:\n",
    "\n",
    "$$\n",
    "w \\in \\mathbb{R}^{600}, \\quad b \\in \\mathbb{R}\n",
    "$$\n",
    "\n",
    "### **Step 2: Forward Pass**\n",
    "For each query-passage pair, compute the probability of relevance:\n",
    "\n",
    "$$\n",
    "z = w^T x + b\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "### **Step 3: Compute Loss**\n",
    "The **binary cross-entropy loss** measures how well the model's predicted probabilities match the ground truth:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = - \\frac{1}{N} \\sum_{i=1}^{N} y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)\n",
    "$$\n",
    "\n",
    "### **Step 4: Compute Gradients**\n",
    "We update the parameters using **gradient descent**:\n",
    "\n",
    "- **Gradient of the loss with respect to weights:**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w} = \\frac{1}{N} \\sum_{i=1}^{N} x_i (\\hat{y}_i - y_i)\n",
    "$$\n",
    "\n",
    "- **Gradient of the loss with respect to bias:**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b} = \\frac{1}{N} \\sum_{i=1}^{N} (\\hat{y}_i - y_i)\n",
    "$$\n",
    "\n",
    "### **Step 5: Update Weights**\n",
    "Using **gradient descent**, update $ w $ and $ b $:\n",
    "\n",
    "$$\n",
    "w = w - \\eta \\frac{\\partial \\mathcal{L}}{\\partial w}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b = b - \\eta \\frac{\\partial \\mathcal{L}}{\\partial b}\n",
    "$$\n",
    "\n",
    "where $ \\eta $ is the **learning rate**.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Evaluating the Model**\n",
    "Once the model is trained, we evaluate it using **ranking metrics**.\n",
    "\n",
    "### **Step 1: Compute Predictions**\n",
    "For each query-passage pair, we obtain a probability $ \\hat{y} $ and predict relevance:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\sigma(w^T x + b)\n",
    "$$\n",
    "\n",
    "If $ \\hat{y} \\geq 0.5 $, classify as **relevant (1)**, otherwise **not relevant (0)**.\n",
    "\n",
    "### **Step 2: Compute AP and NDCG**\n",
    "We use **Average Precision (AP) and NDCG** to measure ranking performance.\n",
    "\n",
    "- **Average Precision (AP):**\n",
    "\n",
    "$$\n",
    "AP = \\frac{1}{R} \\sum_{k=1}^{N} P(k) \\cdot rel(k)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ P(k) $ = Precision at rank $ k $\n",
    "- $ rel(k) $ = 1 if the passage at rank $ k $ is relevant\n",
    "\n",
    "- **NDCG@k:**\n",
    "\n",
    "$$\n",
    "DCG@k = \\sum_{i=1}^{k} \\frac{rel_i}{\\log_2(i+1)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "NDCG@k = \\frac{DCG@k}{IDCG@k}\n",
    "$$\n",
    "\n",
    "where $ IDCG@k $ is the ideal ranking.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Analyzing Learning Rate**\n",
    "The learning rate $ \\eta $ controls how much we update $ w $ and $ b $ at each step.\n",
    "\n",
    "### **Effects of Learning Rate:**\n",
    "1. **Too small ($ \\eta \\to 0 $)** → Slow convergence\n",
    "2. **Too large ($ \\eta \\to 1 $)** → Instability, oscillations\n",
    "3. **Optimal ($ \\eta \\approx 0.01 $)** → Smooth loss decrease\n",
    "\n",
    "To analyze this, we plot **loss vs. epochs** for different learning rates:\n",
    "\n",
    "- $ \\eta = 0.001 $\n",
    "- $ \\eta = 0.01 $\n",
    "- $ \\eta = 0.1 $\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Summary**\n",
    "✔ **Implemented Logistic Regression theory**  \n",
    "✔ **Defined feature representation using embeddings**  \n",
    "✔ **Derived gradient descent updates**  \n",
    "✔ **Explained evaluation using AP & NDCG**  \n",
    "✔ **Analyzed learning rate effects**\n",
    "\n",
    "---\n",
    "\n",
    "# **Numerical Example: Logistic Regression for Passage Relevance**\n",
    "Let’s go step by step through a **manual numerical example** to illustrate how logistic regression predicts passage relevance.\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 1: Define a Small Feature Space**\n",
    "Instead of using **600-dimensional embeddings**, we use a simple **3D feature space** for easier calculations.\n",
    "\n",
    "Suppose we have the following **query and passage embeddings**:\n",
    "\n",
    "| Feature | Query Embedding ($E_q$) | Passage Embedding ($E_p$) |\n",
    "|---------|----------------|----------------|\n",
    "| $x_1$  | **0.2**        | **0.4**        |\n",
    "| $x_2$  | **0.3**        | **0.7**        |\n",
    "| $x_3$  | **0.5**        | **0.6**        |\n",
    "\n",
    "The **feature vector $x$** is formed by concatenating query and passage embeddings:\n",
    "\n",
    "$$\n",
    "x = [E_q \\oplus E_p] = [0.2, 0.3, 0.5, 0.4, 0.7, 0.6]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 2: Initialize Logistic Regression Parameters**\n",
    "We initialize **weights $w$** and **bias $b$**:\n",
    "\n",
    "$$\n",
    "w = [0.1, 0.2, -0.1, 0.05, -0.3, 0.4]\n",
    "$$\n",
    "\n",
    "$$\n",
    "b = 0.05\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 3: Compute Weighted Sum ($z$)**\n",
    "The model computes:\n",
    "\n",
    "$$\n",
    "z = w^T x + b\n",
    "$$\n",
    "\n",
    "$$\n",
    "= (0.1 \\times 0.2) + (0.2 \\times 0.3) + (-0.1 \\times 0.5) + (0.05 \\times 0.4) + (-0.3 \\times 0.7) + (0.4 \\times 0.6) + 0.05\n",
    "$$\n",
    "\n",
    "$$\n",
    "= (0.02) + (0.06) + (-0.05) + (0.02) + (-0.21) + (0.24) + 0.05\n",
    "$$\n",
    "\n",
    "$$\n",
    "= -0.07\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 4: Apply the Sigmoid Function**\n",
    "We compute the probability:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{1}{1 + e^{0.07}}\n",
    "$$\n",
    "\n",
    "Approximating:\n",
    "\n",
    "$$\n",
    "e^{0.07} \\approx 1.0725\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\frac{1}{1 + 1.0725} = \\frac{1}{2.0725} \\approx 0.483\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 5: Predict Passage Relevance**\n",
    "We use a **threshold of 0.5**:\n",
    "- If $ \\hat{y} \\geq 0.5 $, classify as **relevant (1)**.\n",
    "- If $ \\hat{y} < 0.5 $, classify as **not relevant (0)**.\n",
    "\n",
    "Since $ \\hat{y} = 0.483 $, the model **predicts that the passage is NOT relevant (0).**\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 6: Compute the Loss (Binary Cross-Entropy)**\n",
    "Assume the **true relevance label** is $ y = 1 $ (meaning the passage is actually relevant).\n",
    "\n",
    "The **Binary Cross-Entropy Loss** is:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = - y \\log(\\hat{y}) - (1 - y) \\log(1 - \\hat{y})\n",
    "$$\n",
    "\n",
    "$$\n",
    "= - (1 \\times \\log(0.483)) - (0 \\times \\log(1 - 0.483))\n",
    "$$\n",
    "\n",
    "$$\n",
    "= - \\log(0.483)\n",
    "$$\n",
    "\n",
    "Approximating:\n",
    "\n",
    "$$\n",
    "\\log(0.483) \\approx -0.315\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = 0.315\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 7: Compute Gradients for Weight Updates**\n",
    "To update $ w $ and $ b $, we compute the gradients:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w} = x (\\hat{y} - y)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b} = (\\hat{y} - y)\n",
    "$$\n",
    "\n",
    "Since:\n",
    "\n",
    "$$\n",
    "\\hat{y} - y = 0.483 - 1 = -0.517\n",
    "$$\n",
    "\n",
    "We compute the weight updates:\n",
    "\n",
    "$$\n",
    "\\Delta w = -0.517 \\times x = [-0.103, -0.155, -0.259, -0.207, -0.362, -0.31]\n",
    "$$\n",
    "\n",
    "Bias update:\n",
    "\n",
    "$$\n",
    "\\Delta b = -0.517\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 8: Update Weights (Gradient Descent)**\n",
    "Using **learning rate** $ \\eta = 0.01 $:\n",
    "\n",
    "$$\n",
    "w = w - \\eta \\Delta w\n",
    "$$\n",
    "\n",
    "$$\n",
    "b = b - \\eta \\Delta b\n",
    "$$\n",
    "\n",
    "$$\n",
    "w = [0.1, 0.2, -0.1, 0.05, -0.3, 0.4] - 0.01 \\times [-0.103, -0.155, -0.259, -0.207, -0.362, -0.31]\n",
    "$$\n",
    "\n",
    "$$\n",
    "= [0.10103, 0.20155, -0.09741, 0.05207, -0.29638, 0.4031]\n",
    "$$\n",
    "\n",
    "$$\n",
    "b = 0.05 - 0.01 \\times (-0.517) = 0.050517\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## **Final Summary**\n",
    "1. **Initial prediction:** $ \\hat{y} = 0.483 \\Rightarrow $ Model predicts **not relevant (0)**.\n",
    "2. **True label:** $ y = 1 $ (actual passage is relevant).\n",
    "3. **Loss:** $ 0.315 $ (cross-entropy loss).\n",
    "4. **Weight updates:** Weights and bias are **slightly updated** after **gradient descent**.\n",
    "\n",
    "---\n",
    "\n",
    "# **LambdaMART: Learning to Rank with XGBoost**\n",
    "\n",
    "## **1. What is LambdaMART?**\n",
    "LambdaMART is a **gradient-boosted tree-based ranking algorithm**, combining:\n",
    "- **LambdaRank** (a ranking optimization method using pairwise relevance differences)\n",
    "- **MART (Multiple Additive Regression Trees)** (a boosting-based tree ensemble model)\n",
    "\n",
    "LambdaMART is **superior to Logistic Regression** because:\n",
    "✔ It captures **non-linear relationships** between features.  \n",
    "✔ It optimizes **ranking metrics** directly (NDCG, MAP).  \n",
    "✔ It can learn from **graded relevance labels** (not just binary labels).  \n",
    "\n",
    "---\n",
    "\n",
    "## **2. Input Representation for LambdaMART**\n",
    "To train the **LambdaMART ranking model**, we need:\n",
    "\n",
    "1. **Feature Vectors:**  \n",
    "   - **Query features**: Query embedding (300D)\n",
    "   - **Passage features**: Passage embedding (300D)\n",
    "   - **BM25 score** (optional)\n",
    "   - **Query-Passage similarity features** (e.g., cosine similarity)\n",
    "\n",
    "2. **Relevance Scores:**  \n",
    "   - Instead of **binary labels (0/1)** from Logistic Regression, we can use **graded relevance scores** (if available).\n",
    "   - If only **binary labels** exist, we can still train the model effectively.\n",
    "\n",
    "Final **feature vector (X) format**:\n",
    "$$\n",
    "X = [E_q \\oplus E_p, \\text{BM25 score}, \\text{Cosine Similarity}]\n",
    "$$\n",
    "Where:\n",
    "- $ E_q $ → Query embedding (300D)\n",
    "- $ E_p $ → Passage embedding (300D)\n",
    "- **BM25 Score (1D)**\n",
    "- **Cosine Similarity (1D)**  \n",
    "\n",
    "Thus, **input size = 600 + 2 = 602 dimensions**.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Training LambdaMART with XGBoost**\n",
    "XGBoost allows **LambdaMART training** by setting:\n",
    "\n",
    "$$\n",
    "\\text{objective} = \"rank:ndcg\"\n",
    "$$\n",
    "\n",
    "This tells XGBoost to **optimize NDCG (Normalized Discounted Cumulative Gain)** directly.\n",
    "\n",
    "### **Training Setup**\n",
    "- **Query Groups**:  \n",
    "  - XGBoost requires queries to be grouped so it **knows which passages belong to which query**.\n",
    "- **Hyperparameter Tuning**:  \n",
    "  - Important parameters include:\n",
    "    - `learning_rate`: Step size in gradient boosting.\n",
    "    - `n_estimators`: Number of boosting rounds.\n",
    "    - `max_depth`: Tree depth.\n",
    "    - `min_child_weight`: Minimum samples required to split a node.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. LambdaMART Model Training Pipeline**\n",
    "### **Step 1: Preprocessing**\n",
    "- Compute **query and passage embeddings** using **Word2Vec, GloVe, or FastText**.\n",
    "- Compute **BM25 scores** (if available).\n",
    "- Compute **Cosine Similarity**:\n",
    "\n",
    "$$\n",
    "\\text{CosineSim}(E_q, E_p) = \\frac{E_q \\cdot E_p}{||E_q|| \\cdot ||E_p||}\n",
    "$$\n",
    "\n",
    "- Form feature matrix **$X$** (size **$N \\times 602$**) and corresponding **relevance labels**.\n",
    "\n",
    "### **Step 2: Define Query Groups**\n",
    "- XGBoost requires knowing how many **passages belong to each query**.\n",
    "- Example: If **Query 1 has 3 passages** and **Query 2 has 2 passages**, the query group is:\n",
    "\n",
    "$$\n",
    "\\text{query_groups} = [3, 2]\n",
    "$$\n",
    "\n",
    "### **Step 3: Train LambdaMART**\n",
    "- Set $ \\text{objective}=\"rank:ndcg\" $\n",
    "- Tune **hyperparameters** (learning rate, max depth, etc.).\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Hyperparameter Tuning Methodology**\n",
    "We optimize the following hyperparameters:\n",
    "\n",
    "| Hyperparameter | Description |\n",
    "|---------------|-------------|\n",
    "| `learning_rate` | Step size for boosting (e.g., 0.01, 0.05, 0.1) |\n",
    "| `n_estimators` | Number of boosting rounds (e.g., 100, 200, 500) |\n",
    "| `max_depth` | Maximum depth of trees (e.g., 3, 5, 7) |\n",
    "| `min_child_weight` | Minimum samples required to split a node |\n",
    "| `subsample` | Fraction of data used per boosting round (0.7-1.0) |\n",
    "\n",
    "### **Grid Search Strategy**\n",
    "1. **Step 1:** Train models with different `learning_rate` values: **(0.01, 0.05, 0.1)**\n",
    "2. **Step 2:** For the best `learning_rate`, tune `max_depth`: **(3, 5, 7)**\n",
    "3. **Step 3:** Optimize `n_estimators`: **(100, 200, 500)**\n",
    "4. **Step 4:** Fine-tune `min_child_weight` and `subsample`\n",
    "\n",
    "We evaluate each model using **NDCG@10** on the **validation set**.\n",
    "\n",
    "---\n",
    "\n",
    "# **6. Numerical Example**\n",
    "Let's assume we have **one query with three passages**, each represented by three features.\n",
    "\n",
    "| Query ID | Passage ID | Feature 1 ($x_1$) | Feature 2 ($x_2$) | Feature 3 ($x_3$) | Relevance ($y$) |\n",
    "|----------|-----------|----------------|----------------|----------------|----------------|\n",
    "| 1        | 101       | **0.2**        | **0.5**        | **0.7**        | **2** |\n",
    "| 1        | 102       | **0.1**        | **0.4**        | **0.6**        | **1** |\n",
    "| 1        | 103       | **0.3**        | **0.2**        | **0.8**        | **0** |\n",
    "\n",
    "Here, **higher $y$ values indicate higher relevance**.\n",
    "\n",
    "### **Step 1: Compute Pairwise Differences**\n",
    "LambdaMART optimizes ranking by computing **gradient updates** for **pairwise differences**:\n",
    "\n",
    "- **Compare (101, 102)** → **101 should be ranked higher**.\n",
    "- **Compare (101, 103)** → **101 should be ranked higher**.\n",
    "- **Compare (102, 103)** → **102 should be ranked higher**.\n",
    "\n",
    "### **Step 2: Compute NDCG**\n",
    "Assume the model initially ranks the passages as:\n",
    "\n",
    "| Rank | Passage ID | Model Score |\n",
    "|------|-----------|--------------|\n",
    "| 1    | 103       | **0.9** |\n",
    "| 2    | 102       | **0.7** |\n",
    "| 3    | 101       | **0.5** |\n",
    "\n",
    "The **DCG (Discounted Cumulative Gain)** is:\n",
    "\n",
    "$$\n",
    "DCG = \\sum_{i=1}^{k} \\frac{rel_i}{\\log_2(i+1)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "DCG = \\frac{0}{\\log_2(1+1)} + \\frac{1}{\\log_2(2+1)} + \\frac{2}{\\log_2(3+1)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 0 + \\frac{1}{1.585} + \\frac{2}{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 0 + 0.63 + 1 = 1.63\n",
    "$$\n",
    "\n",
    "The **ideal ranking (IDCG)** would have **passage 101 at the top**, so:\n",
    "\n",
    "$$\n",
    "NDCG = \\frac{DCG}{IDCG}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "# **7. Final Summary**\n",
    "✔ **Implemented LambdaMART using XGBoost**  \n",
    "✔ **Defined input representation (query embeddings, passage embeddings, BM25, cosine similarity)**  \n",
    "✔ **Explained hyperparameter tuning strategy**  \n",
    "✔ **Demonstrated a simple numerical example with NDCG calculation**\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b19e8b8",
   "metadata": {},
   "source": [
    "# **Neural Network Model for Passage Re-Ranking with Binary Relevance Scores**\n",
    "\n",
    "## **1. Problem Definition**\n",
    "We train a **neural network model** to **re-rank passages** based on their **binary relevance scores** (0 or 1).  \n",
    "Unlike LambdaMART, which uses **boosted trees**, we now use **deep learning architectures** such as:\n",
    "- **Feedforward Neural Networks (MLP)**\n",
    "- **CNNs**\n",
    "- **RNNs (LSTM/GRU)**\n",
    "- **Transformers (BERT-based models)**\n",
    "\n",
    "For efficiency, we use **Feedforward MLP** first.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Input Representation**\n",
    "Each query-passage pair is represented as:\n",
    "\n",
    "$$\n",
    "X = [E_q \\oplus E_p, \\text{BM25 score}, \\text{Cosine Similarity}]\n",
    "$$\n",
    "\n",
    "where:\n",
    "- **$E_q$** → Query embedding (300D)\n",
    "- **$E_p$** → Passage embedding (300D)\n",
    "- **BM25 Score (1D)**\n",
    "- **Cosine Similarity (1D)**  \n",
    "\n",
    "Total **input size**: **602 dimensions**.\n",
    "\n",
    "The **label ($Y$)** is **binary relevance**:  \n",
    "- **1 = relevant**\n",
    "- **0 = not relevant**\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Choosing the Neural Network Architecture**\n",
    "We choose a **Feedforward Neural Network (MLP)** with:\n",
    "1. **Input Layer (602D)**\n",
    "2. **Hidden Layers (Dense Fully Connected Layers)**\n",
    "   - **ReLU activations** for non-linearity.\n",
    "   - **Dropout** for regularization.\n",
    "3. **Output Layer (Sigmoid Activation)**\n",
    "   - Outputs a **probability score** (0 to 1).\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Mathematical Formulation**\n",
    "Given input **$X$**, the neural network computes:\n",
    "\n",
    "$$\n",
    "h_1 = \\text{ReLU}(W_1 X + b_1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_2 = \\text{ReLU}(W_2 h_1 + b_2)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\sigma(W_3 h_2 + b_3)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ W_1, W_2, W_3 $ = Trainable weight matrices\n",
    "- $ b_1, b_2, b_3 $ = Bias terms\n",
    "- $ \\sigma(z) $ = **Sigmoid function** to get a probability score.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Loss Function: Binary Cross-Entropy**\n",
    "Since **relevance is binary (0 or 1)**, we use **Binary Cross-Entropy Loss**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = - \\frac{1}{N} \\sum_{i=1}^{N} y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ y_i \\in \\{0,1\\} $ is the **ground truth** (1 = relevant, 0 = not relevant).\n",
    "- $ \\hat{y}_i $ is the **predicted probability of relevance**.\n",
    "\n",
    "The goal is to:\n",
    "✔ **Maximize** probability for **relevant passages ($y = 1$)**.  \n",
    "✔ **Minimize** probability for **non-relevant passages ($y = 0$)**.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Evaluation Metrics**\n",
    "After training, evaluate model performance using **ranking metrics**:\n",
    "1. **NDCG@k** (Ranking Quality)\n",
    "2. **MAP (Mean Average Precision)**\n",
    "\n",
    "To compute **NDCG**, sort passages **by predicted probability** and calculate:\n",
    "\n",
    "$$\n",
    "DCG@k = \\sum_{i=1}^{k} \\frac{rel_i}{\\log_2(i+1)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "NDCG@k = \\frac{DCG@k}{IDCG@k}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "# **7. Numerical Example**\n",
    "### **Step 1: Example Query-Passage Pairs**\n",
    "Assume we have **one query with three passages**, each represented by three features:\n",
    "\n",
    "| Query ID | Passage ID | Feature 1 ($x_1$) | Feature 2 ($x_2$) | Feature 3 ($x_3$) | Relevance ($y$) |\n",
    "|----------|-----------|----------------|----------------|----------------|----------------|\n",
    "| 1        | 101       | **0.2**        | **0.5**        | **0.7**        | **1** |\n",
    "| 1        | 102       | **0.1**        | **0.4**        | **0.6**        | **1** |\n",
    "| 1        | 103       | **0.3**        | **0.2**        | **0.8**        | **0** |\n",
    "\n",
    "### **Step 2: Compute Model Predictions**\n",
    "The neural network assigns probability scores:\n",
    "\n",
    "| Rank | Passage ID | Model Score ($\\hat{y}$) |\n",
    "|------|-----------|--------------|\n",
    "| 1    | 103       | **0.9** |\n",
    "| 2    | 102       | **0.7** |\n",
    "| 3    | 101       | **0.4** |\n",
    "\n",
    "However, this ranking is **incorrect**, as passages **101 and 102** are **more relevant** than **103**.\n",
    "\n",
    "### **Step 3: Compute NDCG**\n",
    "We calculate **DCG** based on predicted ranking:\n",
    "\n",
    "$$\n",
    "DCG = \\sum_{i=1}^{k} \\frac{rel_i}{\\log_2(i+1)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "DCG = \\frac{0}{\\log_2(1+1)} + \\frac{1}{\\log_2(2+1)} + \\frac{1}{\\log_2(3+1)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 0 + \\frac{1}{1.585} + \\frac{1}{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 0 + 0.63 + 0.5 = 1.13\n",
    "$$\n",
    "\n",
    "The **ideal ranking (IDCG)** would have **passages 101 and 102 ranked first**, giving:\n",
    "\n",
    "$$\n",
    "IDCG = 1 + \\frac{1}{\\log_2(2+1)} + \\frac{0}{\\log_2(3+1)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 1 + \\frac{1}{1.585} + 0 = 1.63\n",
    "$$\n",
    "\n",
    "Thus, **NDCG is**:\n",
    "\n",
    "$$\n",
    "NDCG = \\frac{DCG}{IDCG} = \\frac{1.13}{1.63} = 0.69\n",
    "$$\n",
    "\n",
    "The model’s ranking is **not optimal** (NDCG is **0.69 instead of 1**), indicating **incorrect ordering**.\n",
    "\n",
    "---\n",
    "\n",
    "## **8. Final Summary**\n",
    "✔ **Designed a Feedforward MLP for passage ranking**  \n",
    "✔ **Defined loss function (Binary Cross-Entropy) for binary labels**  \n",
    "✔ **Explained feature representation (query + passage embeddings, BM25, cosine similarity)**  \n",
    "✔ **Provided a numerical example with ranking computation using NDCG**  \n",
    "✔ **Compared predictions with the ideal ranking**  \n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
